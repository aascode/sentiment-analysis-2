{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpAtmXkvvWzi",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# **Sentiment Analysis**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF_IfHaPNDmH",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# **1. Installation**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzL6TXxjOpdG",
        "colab_type": "text"
      },
      "source": [
        "## i. Generating a reponse\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvSlAEpfQzo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import logging\n",
        "from psutil import virtual_memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HY2EDdOQxZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "ram_gb = virtual_memory().total / 1e9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xrnkV_nQ2dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_response = {\n",
        "    'error': None,\n",
        "    'TF version': '',\n",
        "    'COLAB': None,\n",
        "    'GPU': False,\n",
        "    'ram_gb': ''\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_99wr64YOQpc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd784969-5cf9-4b44-9b2a-df744cd8b1c8"
      },
      "source": [
        "try:\n",
        "    # drive\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "    # updating tensorflow version\n",
        "    %tensorflow_version 2.x\n",
        "\n",
        "    # tensorflow-gpu\n",
        "    !pip install tensorflow-gpu # !pip install tensorflow_text # I could use BERT\n",
        "    \n",
        "    # NLP (nltk, stanza, spacy)\n",
        "    !pip install nltk \n",
        "    !pip install stanza\n",
        "    !pip install spacy\n",
        "    !spacy download en_core_web_sm # sm md lg\n",
        "    !python -m spacy download en\n",
        "except OSError as error:\n",
        "    # debugging error\n",
        "    response['error'] = logging.debug('You are not using your specify version of TensorFlow')\n",
        "    IN_COLAB = False\n",
        "\n",
        "    # install requirements\n",
        "    !pip install -r '../requirements.txt'\n",
        "finally:\n",
        "    tf_response['COLAB'] = IN_COLAB\n",
        "    \n",
        "    # Importing tensroflow core\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    \n",
        "    # GPU and RAM response\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        GPU = True\n",
        "        tf_response['GPU'] = GPU\n",
        "        tf_response['TF_version'] = tf.__version__\n",
        "        \n",
        "        if tf_response['COLAB'] == True:\n",
        "            if gpu_info.find('failed') >= 0:\n",
        "                print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator')\n",
        "                print('Re-execute this cell.')\n",
        "            else:\n",
        "                print(gpu_info)\n",
        "            \n",
        "            if ram_gb < 20:\n",
        "                print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type menu\"')\n",
        "                print('Select high-RAM in the runtime shape dropdown')\n",
        "                print('Re-execute this cell')\n",
        "                tf_response['ram_gb'] = 'low-RAM runtime'\n",
        "            else:\n",
        "                tf_response['ram_gb'] = 'high-RAM runtime'\n",
        "            print('\\nRuntime {:.2f} GB of available RAM\\n'.format(ram_gb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/11/763f55d3d15efd778ef24453f126e6c33635680e5a2bb346da3fab5997cb/tensorflow_gpu-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 44kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.35.1)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (49.6.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.3.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/8b/3a9e7a8d8cb14ad6afffc3983b7a7322a3a24d94ebc978a70746fcffc085/stanza-1.1.1-py3-none-any.whl (227kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.6.0+cu101)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (49.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.1.1\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (49.6.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (49.6.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Thu Aug 27 04:54:56 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n",
            "Runtime 27.39 GB of available RAM\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7s0vMHrRS59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "4b5dcabc-85b7-4753-829a-cc5149b20a78"
      },
      "source": [
        "tf_response"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'COLAB': True,\n",
              " 'GPU': True,\n",
              " 'TF version': '',\n",
              " 'TF_version': '2.3.0',\n",
              " 'error': None,\n",
              " 'ram_gb': 'high-RAM runtime'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF6hVyvJSEN9",
        "colab_type": "text"
      },
      "source": [
        "## ii. Importing modules\n",
        "\n",
        "> Bloque con sangría\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW1C6W9DQws8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data analysis\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import re\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator # to create a Word Cloud\n",
        "from PIL import Image # Pillow with WordCloud to image manipulation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBIi3kzrV3ak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "288a47eb-e523-4111-86da-eb6415347124"
      },
      "source": [
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sweo4LneWAfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "97dc1ff6-5b7a-482b-db38-192b517847c1"
      },
      "source": [
        "# Stanza NLP\n",
        "import stanza\n",
        "\n",
        "stanza.download('en', package='ewt', processors='tokenize,mwt,pos,lemma', verbose=True)\n",
        "stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma',\n",
        "                      lang='en',\n",
        "                      use_gpu=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 13.2MB/s]                    \n",
            "2020-08-27 04:56:03 WARNING: Can not find mwt: ewt from official model list. Ignoring it.\n",
            "2020-08-27 04:56:03 INFO: Downloading these customized packages for language: en (English)...\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "| pretrain  | ewt     |\n",
            "=======================\n",
            "\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/tokenize/ewt.pt: 100%|██████████| 631k/631k [00:00<00:00, 1.07MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/pos/ewt.pt: 100%|██████████| 22.1M/22.1M [00:01<00:00, 12.0MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/lemma/ewt.pt: 100%|██████████| 3.36M/3.36M [00:01<00:00, 3.03MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/pretrain/ewt.pt: 100%|██████████| 156M/156M [00:32<00:00, 4.86MB/s]\n",
            "2020-08-27 04:56:42 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2020-08-27 04:56:42 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
            "2020-08-27 04:56:42 INFO: Loading these models for language: en (English):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "=======================\n",
            "\n",
            "2020-08-27 04:56:42 INFO: Use device: gpu\n",
            "2020-08-27 04:56:42 INFO: Loading: tokenize\n",
            "2020-08-27 04:56:52 INFO: Loading: pos\n",
            "2020-08-27 04:56:53 INFO: Loading: lemma\n",
            "2020-08-27 04:56:53 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zH74LpiW1aM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "03d54179-969a-47e3-a60b-1a73a4df4740"
      },
      "source": [
        "# testing stanza\n",
        "doc = stNLP('Barack Obama was born in Hawai.')\n",
        "print('\\n')\n",
        "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "word: Barack \tlemma: Barack\n",
            "word: Obama \tlemma: Obama\n",
            "word: was \tlemma: be\n",
            "word: born \tlemma: bear\n",
            "word: in \tlemma: in\n",
            "word: Hawai \tlemma: Hawai\n",
            "word: . \tlemma: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aacz6_0m5xvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spacy NLP\n",
        "import spacy\n",
        "spNLP = spacy.load('en_core_web_sm')\n",
        "spNLP.max_length = 103950039 # or higher\n",
        "# spacy.prefer_gpu() #will not work with stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh1cTaEhS0aP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# **2. Hyperparameters**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW0G_4k17U18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 30\n",
        "vocab_size = 5000\n",
        "embedding_dim = 64\n",
        "max_lenght = 200\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = '<OOV>'\n",
        "training_portion = .8\n",
        "\n",
        "PATH_FILE = 'data/training.1600000.processed.noemoticon.csv'\n",
        "FILE = 'data/sentiment-subset.csv'\n",
        "main_labels = ['happy', 'confident', 'sad', 'not confident']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrPWPhX9TDs4",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# **3. Lemmatization**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npocB7yHDfHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lemmatizion\n",
        "# stanza\n",
        "def stanza_lemma(text):\n",
        "    doc = stNLP(text)\n",
        "    return ' '.join([word.lemma for sent in doc.sentences for word in sent.words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHpMteO-TKh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nltk_lemma(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatizer.lemmatize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dZXQ6kgTLs8",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# **4. Preprocessing & load dataset**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhZ1AG0GTZxM",
        "colab_type": "text"
      },
      "source": [
        "### i. Load database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G21PzPJFYEAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_path=None):\n",
        "    print('load the dataset...\\n')\n",
        "    !mkdir -p data\n",
        "    !wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip -P data\n",
        "    !unzip -n -d data data/training.1600000.processed.noemoticon.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny2Bv6oWTkzE",
        "colab_type": "text"
      },
      "source": [
        "### ii. Preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wO_zeUcFHUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_dataset():\n",
        "    print('preprocess the dataset...\\n')\n",
        "\n",
        "    # load_data\n",
        "    load_data()\n",
        "    print('Database loaded\\n')\n",
        "\n",
        "    # cleaning data\n",
        "    unclean_df = pd.read_csv(PATH_FILE,\n",
        "                     names=['polarity', 'id', 'date', 'query', 'user', 'text'],\n",
        "                     encoding='latin-1') # if utf-8: UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 232719-232720: invalid continuation byte\n",
        "\n",
        "    unclean_df.polarity = unclean_df.polarity.replace({0: 0, 4: 1}) # replace polarity\n",
        "    unclean_df = unclean_df.drop(columns=['id', 'date', 'query', 'user']) # dropping unneeded columns\n",
        "\n",
        "    # sample\n",
        "    #df_sample = unclean_df.sample(n=500000)\n",
        "    #df_sample.polarity.value_counts()\n",
        "\n",
        "    # lower case\n",
        "    unclean_df['text'] = unclean_df['text'].str.lower()\n",
        "\n",
        "    # remove character and numbers\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'https://www\\.|http:\\.|https://|www\\.', '', x))\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil|cl)[\\S]*\\s?', '', x))\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-zÁ-Úá-ú \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?%', '', x))\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "\n",
        "    # rewritting the created file without NaN values\n",
        "    unclean_df.to_csv('data/sentiment140-subset.csv', \n",
        "              quotechar='\"', # check later!\n",
        "              encoding='latin-1',\n",
        "              index=False)\n",
        "\n",
        "    # clean csv\n",
        "    df = pd.read_csv('data/sentiment140-subset.csv', encoding='latin-1').dropna()\n",
        "\n",
        "    # checking if there's any NaN values\n",
        "    isnull = [i for i in (df['text'].isnull()) if i == True]\n",
        "    if isnull != []:\n",
        "        sys.exit(0) # add response object here\n",
        "\n",
        "    \n",
        "    # STOPWORDS\n",
        "    # Getting in a list all the stopwords of the dataframe\n",
        "    #spacy_stop_words = list(dict.fromkeys([str(i) for i in spNLP(' '.join(j for j in df['text'])) if i.is_stop == True]))\n",
        "    \n",
        "    '''\n",
        "    stop_words = stopwords.words('english')\n",
        "    stop_words.extend(spacy_stop_words)\n",
        "    stop_words = set(stop_words)\n",
        "    \n",
        "    df['text'] = df['text'].apply(lambda x: ' '.join([i for i in x.split() if i not in stop_words]))\n",
        "    \n",
        "    # Lemmatization Stanza vs NLTK\n",
        "    df['text'] = df['text'].apply(lambda x: stanza_lemma(x))\n",
        "    df['text'] = df['text'].apply(lambda x: nltk_lemma(x))\n",
        "\n",
        "    # check new stopwords here!\n",
        "    df['text'] = df['text'].apply(lambda x: ' '.join(\n",
        "        [i for i in x.split() if i not in stop_words]\n",
        "    ))\n",
        "    '''\n",
        "    # return df\n",
        "    x, y = df.text, df.polarity\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtGPDu4aFxQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "d4718771-8a0f-44dd-99e9-9bfd004f3497"
      },
      "source": [
        "# test peprocess_dataset\n",
        "x, y = preprocess_dataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocess the dataset...\n",
            "\n",
            "load the dataset...\n",
            "\n",
            "--2020-08-27 04:56:56--  https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip\n",
            "Resolving nyc3.digitaloceanspaces.com (nyc3.digitaloceanspaces.com)... 162.243.189.2\n",
            "Connecting to nyc3.digitaloceanspaces.com (nyc3.digitaloceanspaces.com)|162.243.189.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85088192 (81M) [application/zip]\n",
            "Saving to: ‘data/training.1600000.processed.noemoticon.csv.zip’\n",
            "\n",
            "training.1600000.pr 100%[===================>]  81.15M  82.0MB/s    in 1.0s    \n",
            "\n",
            "2020-08-27 04:56:57 (82.0 MB/s) - ‘data/training.1600000.processed.noemoticon.csv.zip’ saved [85088192/85088192]\n",
            "\n",
            "Archive:  data/training.1600000.processed.noemoticon.csv.zip\n",
            "  inflating: data/training.1600000.processed.noemoticon.csv  \n",
            "Database loaded\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJqblmY3KNJJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "b06d9fe5-316e-4bbd-bff2-d244eb47daab"
      },
      "source": [
        "# test peprocess_dataset\n",
        "print(y.value_counts())\n",
        "y.value_counts().plot(kind='bar')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    799982\n",
            "1    799969\n",
            "Name: polarity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD1CAYAAAClSgmzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUdElEQVR4nO3df6zd9X3f8eeruLQ0XYJD7ixqmxkpXiMHKQlY4CrTtIXVGDrV/JFEoGlYyIonhWzNMmlx9o81GBKRprEiJZas4sWeuhCXNcLKnLiWk6iqJhNfEgYBynxLQmyLH7e2A2uzJCV974/z8XK4OZ9zjwmc6+DnQzo63+/78/l8P58jXZ2Xz/f7PT6pKiRJGuWXlnoBkqTzlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqSuZUu9gNfbO97xjlqzZs1SL0OSfqE88sgjf1lVMwvrb7qQWLNmDbOzs0u9DEn6hZLk2VF1TzdJkroMCUlSlyEhSeoyJCRJXYaEJKlropBI8q+TPJHk20k+n+RXk1yZ5OEkc0m+kOTi1vdX2v5ca18zdJxPtfrTSW4Yqm9qtbkk24fqI+eQJE3HoiGRZCXwr4D1VXUVcBFwC/Bp4N6qeidwBtjahmwFzrT6va0fSda1ce8GNgGfTXJRkouAzwA3AuuAW1tfxswhSZqCSU83LQMuSbIM+DXgOeADwIOtfQ9wc9ve3PZp7dcnSas/UFU/qqrvAHPAte0xV1XPVNWPgQeAzW1Mbw5J0hQs+mW6qjqZ5D8C3wP+L/AnwCPA96vqldbtBLCyba8EjrexryR5Cbis1Y8MHXp4zPEF9evamN4cr5JkG7AN4IorrljsJZ0X1mz/H0u9hDeN797zO0u9hDcV/zZfX7/of5+TnG5azuBTwJXAbwBvYXC66LxRVbuqan1VrZ+Z+ZlvlUuSXqNJTjf9E+A7VTVfVX8D/DHwfuDSdvoJYBVwsm2fBFYDtPa3AaeG6wvG9OqnxswhSZqCSULie8CGJL/WrhNcDzwJfA34YOuzBXiobe9v+7T2r9bgh7T3A7e0u5+uBNYC3wCOAmvbnUwXM7i4vb+N6c0hSZqCRUOiqh5mcPH4m8Djbcwu4JPAJ5LMMbh+cH8bcj9wWat/AtjejvMEsI9BwHwFuKOqftKuOXwMOAg8BexrfRkzhyRpCib6X2CragewY0H5GQZ3Ji3s+0PgQ53j3A3cPaJ+ADgwoj5yDknSdPiNa0lSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSuhYNiSS/meTRocfLST6e5O1JDiU51p6Xt/5Jcl+SuSSPJbl66FhbWv9jSbYM1a9J8ngbc1/7mVR6c0iSpmOSny99uqreW1XvBa4BfgB8kcHPkh6uqrXA4bYPcCOD369eC2wDdsLgDZ/Br9tdx+DX5nYMvenvBD4yNG5Tq/fmkCRNwbmebroe+IuqehbYDOxp9T3AzW17M7C3Bo4Alya5HLgBOFRVp6vqDHAI2NTa3lpVR6qqgL0LjjVqDknSFJxrSNwCfL5tr6iq59r288CKtr0SOD405kSrjaufGFEfN4ckaQomDokkFwO/C/zRwrb2CaBex3X9jHFzJNmWZDbJ7Pz8/Bu5DEm6oJzLJ4kbgW9W1Qtt/4V2qoj2/GKrnwRWD41b1Wrj6qtG1MfN8SpVtauq1lfV+pmZmXN4SZKkcc4lJG7lp6eaAPYDZ+9Q2gI8NFS/rd3ltAF4qZ0yOghsTLK8XbDeCBxsbS8n2dDuarptwbFGzSFJmoJlk3RK8hbgt4F/MVS+B9iXZCvwLPDhVj8A3ATMMbgT6naAqjqd5C7gaOt3Z1WdbtsfBT4HXAJ8uT3GzSFJmoKJQqKq/hq4bEHtFIO7nRb2LeCOznF2A7tH1GeBq0bUR84hSZoOv3EtSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6pooJJJcmuTBJH+e5Kkkv5Xk7UkOJTnWnpe3vklyX5K5JI8luXroOFta/2NJtgzVr0nyeBtzX/uta3pzSJKmY9JPEr8PfKWq3gW8B3gK2A4crqq1wOG2D3AjsLY9tgE7YfCGD+wArgOuBXYMvenvBD4yNG5Tq/fmkCRNwaIhkeRtwD8E7geoqh9X1feBzcCe1m0PcHPb3gzsrYEjwKVJLgduAA5V1emqOgMcAja1trdW1ZH2+9h7Fxxr1BySpCmY5JPElcA88F+SfCvJHyR5C7Ciqp5rfZ4HVrTtlcDxofEnWm1c/cSIOmPmkCRNwSQhsQy4GthZVe8D/poFp33aJ4B6/Zc32RxJtiWZTTI7Pz//Ri5Dki4ok4TECeBEVT3c9h9kEBovtFNFtOcXW/tJYPXQ+FWtNq6+akSdMXO8SlXtqqr1VbV+ZmZmgpckSZrEoiFRVc8Dx5P8ZitdDzwJ7AfO3qG0BXiobe8Hbmt3OW0AXmqnjA4CG5MsbxesNwIHW9vLSTa0u5puW3CsUXNIkqZg2YT9/iXwh0kuBp4BbmcQMPuSbAWeBT7c+h4AbgLmgB+0vlTV6SR3AUdbvzur6nTb/ijwOeAS4MvtAXBPZw5J0hRMFBJV9SiwfkTT9SP6FnBH5zi7gd0j6rPAVSPqp0bNIUmaDr9xLUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeqaKCSSfDfJ40keTTLbam9PcijJsfa8vNWT5L4kc0keS3L10HG2tP7HkmwZql/Tjj/XxmbcHJKk6TiXTxL/uKreW1Vnf8Z0O3C4qtYCh9s+wI3A2vbYBuyEwRs+sAO4DrgW2DH0pr8T+MjQuE2LzCFJmoKf53TTZmBP294D3DxU31sDR4BLk1wO3AAcqqrTVXUGOARsam1vraoj7fex9y441qg5JElTMGlIFPAnSR5Jsq3VVlTVc237eWBF214JHB8ae6LVxtVPjKiPm0OSNAXLJuz3D6rqZJK/CxxK8ufDjVVVSer1X95kc7Tg2gZwxRVXvJHLkKQLykSfJKrqZHt+Efgig2sKL7RTRbTnF1v3k8DqoeGrWm1cfdWIOmPmWLi+XVW1vqrWz8zMTPKSJEkTWDQkkrwlyd85uw1sBL4N7AfO3qG0BXiobe8Hbmt3OW0AXmqnjA4CG5MsbxesNwIHW9vLSTa0u5puW3CsUXNIkqZgktNNK4AvtrtSlwH/raq+kuQosC/JVuBZ4MOt/wHgJmAO+AFwO0BVnU5yF3C09buzqk637Y8CnwMuAb7cHgD3dOaQJE3BoiFRVc8A7xlRPwVcP6JewB2dY+0Gdo+ozwJXTTqHJGk6/Ma1JKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqWvikEhyUZJvJflS278yycNJ5pJ8IcnFrf4rbX+uta8ZOsanWv3pJDcM1Te12lyS7UP1kXNIkqbjXD5J/B7w1ND+p4F7q+qdwBlga6tvBc60+r2tH0nWAbcA7wY2AZ9twXMR8BngRmAdcGvrO24OSdIUTBQSSVYBvwP8QdsP8AHgwdZlD3Bz297c9mnt17f+m4EHqupHVfUdYA64tj3mquqZqvox8ACweZE5JElTMOknif8M/Fvgb9v+ZcD3q+qVtn8CWNm2VwLHAVr7S63//68vGNOrj5tDkjQFi4ZEkn8KvFhVj0xhPa9Jkm1JZpPMzs/PL/VyJOlNY5JPEu8HfjfJdxmcCvoA8PvApUmWtT6rgJNt+ySwGqC1vw04NVxfMKZXPzVmjlepql1Vtb6q1s/MzEzwkiRJk1g0JKrqU1W1qqrWMLjw/NWq+mfA14APtm5bgIfa9v62T2v/alVVq9/S7n66ElgLfAM4CqxtdzJd3ObY38b05pAkTcHP8z2JTwKfSDLH4PrB/a1+P3BZq38C2A5QVU8A+4Anga8Ad1TVT9o1h48BBxncPbWv9R03hyRpCpYt3uWnqurrwNfb9jMM7kxa2OeHwIc64+8G7h5RPwAcGFEfOYckaTr8xrUkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpa9GQSPKrSb6R5H8leSLJv2/1K5M8nGQuyRfa71PTfsP6C63+cJI1Q8f6VKs/neSGofqmVptLsn2oPnIOSdJ0TPJJ4kfAB6rqPcB7gU1JNgCfBu6tqncCZ4Ctrf9W4Eyr39v6kWQdcAvwbmAT8NkkFyW5CPgMcCOwDri19WXMHJKkKVg0JGrgr9ruL7dHAR8AHmz1PcDNbXtz26e1X58krf5AVf2oqr4DzDH4/eprgbmqeqaqfgw8AGxuY3pzSJKmYKJrEu1f/I8CLwKHgL8Avl9Vr7QuJ4CVbXslcBygtb8EXDZcXzCmV79szBySpCmYKCSq6idV9V5gFYN/+b/rDV3VOUqyLclsktn5+fmlXo4kvWmc091NVfV94GvAbwGXJlnWmlYBJ9v2SWA1QGt/G3BquL5gTK9+aswcC9e1q6rWV9X6mZmZc3lJkqQxJrm7aSbJpW37EuC3gacYhMUHW7ctwENte3/bp7V/taqq1W9pdz9dCawFvgEcBda2O5kuZnBxe38b05tDkjQFyxbvwuXAnnYX0i8B+6rqS0meBB5I8h+AbwH3t/73A/81yRxwmsGbPlX1RJJ9wJPAK8AdVfUTgCQfAw4CFwG7q+qJdqxPduaQJE3BoiFRVY8B7xtRf4bB9YmF9R8CH+oc627g7hH1A8CBSeeQJE2H37iWJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdU3yG9erk3wtyZNJnkjye63+9iSHkhxrz8tbPUnuSzKX5LEkVw8da0vrfyzJlqH6NUkeb2PuS5Jxc0iSpmOSTxKvAP+mqtYBG4A7kqwDtgOHq2otcLjtA9wIrG2PbcBOGLzhAzuA6xj8JOmOoTf9ncBHhsZtavXeHJKkKVg0JKrquar6Ztv+P8BTwEpgM7CnddsD3Ny2NwN7a+AIcGmSy4EbgENVdbqqzgCHgE2t7a1VdaSqCti74Fij5pAkTcE5XZNIsgZ4H/AwsKKqnmtNzwMr2vZK4PjQsBOtNq5+YkSdMXNIkqZg4pBI8uvAfwc+XlUvD7e1TwD1Oq/tVcbNkWRbktkks/Pz82/kMiTpgjJRSCT5ZQYB8YdV9cet/EI7VUR7frHVTwKrh4avarVx9VUj6uPmeJWq2lVV66tq/czMzCQvSZI0gUnubgpwP/BUVf2noab9wNk7lLYADw3Vb2t3OW0AXmqnjA4CG5MsbxesNwIHW9vLSTa0uW5bcKxRc0iSpmDZBH3eD/xz4PEkj7bavwPuAfYl2Qo8C3y4tR0AbgLmgB8AtwNU1ekkdwFHW787q+p02/4o8DngEuDL7cGYOSRJU7BoSFTVnwHpNF8/on8Bd3SOtRvYPaI+C1w1on5q1BySpOnwG9eSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkrkl+43p3kheTfHuo9vYkh5Ica8/LWz1J7ksyl+SxJFcPjdnS+h9LsmWofk2Sx9uY+9rvXHfnkCRNzySfJD4HbFpQ2w4crqq1wOG2D3AjsLY9tgE7YfCGD+wArgOuBXYMvenvBD4yNG7TInNIkqZk0ZCoqj8FTi8obwb2tO09wM1D9b01cAS4NMnlwA3Aoao6XVVngEPAptb21qo60n4be++CY42aQ5I0Ja/1msSKqnqubT8PrGjbK4HjQ/1OtNq4+okR9XFzSJKm5Oe+cN0+AdTrsJbXPEeSbUlmk8zOz8+/kUuRpAvKaw2JF9qpItrzi61+Elg91G9Vq42rrxpRHzfHz6iqXVW1vqrWz8zMvMaXJEla6LWGxH7g7B1KW4CHhuq3tbucNgAvtVNGB4GNSZa3C9YbgYOt7eUkG9pdTbctONaoOSRJU7JssQ5JPg/8I+AdSU4wuEvpHmBfkq3As8CHW/cDwE3AHPAD4HaAqjqd5C7gaOt3Z1WdvRj+UQZ3UF0CfLk9GDOHJGlKFg2Jqrq103T9iL4F3NE5zm5g94j6LHDViPqpUXNIkqbHb1xLkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSus77kEiyKcnTSeaSbF/q9UjSheS8DokkFwGfAW4E1gG3Jlm3tKuSpAvHeR0SwLXAXFU9U1U/Bh4ANi/xmiTpgrFsqRewiJXA8aH9E8B1Czsl2QZsa7t/leTpKaztQvEO4C+XehHj5NNLvQItkfP+bxN+of4+/96o4vkeEhOpql3ArqVex5tRktmqWr/U65AW8m9zOs73000ngdVD+6taTZI0Bed7SBwF1ia5MsnFwC3A/iVekyRdMM7r001V9UqSjwEHgYuA3VX1xBIv60LjaTydr/zbnIJU1VKvQZJ0njrfTzdJkpaQISFJ6jIkJEld5/WFa01Xkncx+Eb7ylY6CeyvqqeWblWSlpKfJARAkk8y+G9PAnyjPQJ83v9YUeezJLcv9RrezLy7SQAk+d/Au6vqbxbULwaeqKq1S7Myabwk36uqK5Z6HW9Wnm7SWX8L/Abw7IL65a1NWjJJHus1ASumuZYLjSGhsz4OHE5yjJ/+p4pXAO8EPrZkq5IGVgA3AGcW1AP8z+kv58JhSAiAqvpKkr/P4L9nH75wfbSqfrJ0K5MA+BLw61X16MKGJF+f/nIuHF6TkCR1eXeTJKnLkJAkdRkSkqQuQ0KS1GVISJK6/h9dXuTZW07rJgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQHKelS39Zan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1cd282dd-f67c-4e68-9181-44989d7136e3"
      },
      "source": [
        "# test peprocess_dataset\n",
        "print(Counter(x).most_common(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(' ', 2301), (' thanks ', 1572), (' get  followers a day using once you add everyone you are on the train or pay vip ', 1324), (' thank you ', 998), (' i am lost please help me find a good home ', 507), (' me too ', 453), ('good morning ', 444), (' good morning ', 355), (' i know ', 342), ('  ', 310)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t8COv8bTslN",
        "colab_type": "text"
      },
      "source": [
        "### iii. Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci01PpVNYdyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_dataset(test_size=0.2, validation_size=0.2):\n",
        "    print('preparing the dataset...\\n')\n",
        "    \n",
        "    # load dataset\n",
        "    # split dataset (as string into panda.core.series.Serie object)\n",
        "    (x, y) = preprocess_dataset()\n",
        "    \n",
        "    # create/split train, validation and test\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)\n",
        "    x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # pandas.core.series.Series to numpy array\n",
        "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "    x_validation, y_validation =  np.array(x_validation), np.array(y_validation)\n",
        "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
        "\n",
        "    return (x_train, y_train), (x_validation, y_validation), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXKY4MEpMiQ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "0d4c6c74-ac76-41dd-af6b-44eeef45ea06"
      },
      "source": [
        "# test prepare_dataset function\n",
        "(x_train, y_train), (x_validation, y_validation), (x_test, y_test) = prepare_dataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preparing the dataset...\n",
            "\n",
            "preprocess the dataset...\n",
            "\n",
            "load the dataset...\n",
            "\n",
            "File ‘data/training.1600000.processed.noemoticon.csv.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  data/training.1600000.processed.noemoticon.csv.zip\n",
            "Database loaded\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvZkUZ5BO-n-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "def prepare_dataset_testing():\n",
        "    print('preparing the dataset...\\n')\n",
        "           \n",
        "    # test with csv module\n",
        "    labels, texts = [], [] \n",
        "    with open('data/sentiment140-subset.csv', 'r', encoding='latin-1') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            labels.append(row[0])\n",
        "            text = row[1]\n",
        "            for i in STOPWORDS:\n",
        "                token = ' ' + i + ' '\n",
        "                text = text.replace(token, ' ')\n",
        "                text = text.replace(' ', ' ')\n",
        "            texts.append(text)\n",
        "\n",
        "    # split\n",
        "    train_size = int(len(texts) * training_portion)\n",
        "\n",
        "    train_texts = texts[:train_size]\n",
        "    train_labels = labels[:train_size]\n",
        "\n",
        "    validation_texts = texts[train_size:]\n",
        "    validation_labels = labels[train_size:]\n",
        "\n",
        "    # tokenizer\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "\n",
        "    tokenizer.fit_on_texts(train_texts)\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    # text to sequences: triain_texts\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "\n",
        "    # padding and truncating sequences: train_seq\n",
        "    train_padded = pad_sequences(sequences=train_sequences, maxlen=max_lenght,\n",
        "                                 padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    # same process: validation_texts\n",
        "    valdiation_sequences = tokenizer.texts_to_sequences(validation_texts)\n",
        "    validation_padded = pad_sequences(valdiation_sequences, maxlen=max_lenght, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    # tokenize & sequences to train and validation LABELS\n",
        "    label_tokenizer = Tokenizer()\n",
        "    label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "    training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
        "    validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n",
        "\n",
        "    print(label_tokenizer.word_index)\n",
        "\n",
        "    return train_padded, training_label_seq, validation_padded, validation_label_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fet5offGT4gM",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **4. Build model**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iasm0J2eYsvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(learning_rate=0.0001, opt='adam', loss='categorical_crossentropy'):\n",
        "    print('building the model...\\n')\n",
        "\n",
        "    # model\n",
        "    model = Sequential()\n",
        "\n",
        "    # layers\n",
        "    model.add(Embedding(vocab_size, embedding_dim))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Bidirectional(LSTM(embedding_dim)))\n",
        "\n",
        "    # softmax output layer\n",
        "    model.add(tf.keras.layers.Dense(units=2, # 10 \n",
        "                                    activation='softmax'))\n",
        "\n",
        "    # optimizer & loss\n",
        "    opt = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss='sparse_categorical_crossentropy'\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=loss,\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35NcHgPNUHPt",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **6. Train model** \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OaqUi0_ZwH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, x_train, y_train, x_validation, y_validation,\n",
        "          epochs, batch_size=32, patience=5, \n",
        "          verbose=2, monitor='accuracy'):\n",
        "    \n",
        "    print('training...\\n')\n",
        "\n",
        "    # callback\n",
        "    early_callback = tf.keras.callbacks.EarlyStopping(monitor=monitor, # also try 'val_loss'\n",
        "                                                      verbose=1, mode='auto', restore_best_weights=True,\n",
        "                                                      min_delta=1e-3, patience=patience)\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
        "                        validation_data=(x_validation, y_validation), # x_test, y_test\n",
        "                        callbacks=[early_callback])\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9UK7KmKUOQV",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **7. Plotting history**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsuytCdNDyDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history_(history):\n",
        "    fitModel_dict = history.history\n",
        "    acc = fitModel_dict['accuracy']\n",
        "    val_acc = fitModel_dict['val_accuracy']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "    \n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
        "    plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.ylim((0.5, 1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_history(history, string):\n",
        "    fitModel_dict = history.history\n",
        "    plt.plot(fitModel_dict[string])\n",
        "    plt.plot/fitModel_dict['val_' + string]\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_' + string])\n",
        "    plt.show()\n",
        "\n",
        "# test\n",
        "#plot_history(history, 'accuracy')\n",
        "#plot_history(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2lMVAEFUVj9",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **8. Main**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBvUnHHkbykJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # x_train, y_train, x_validation, y_validation, x_test, y_test = prepare_dataset(data_path)\n",
        "    train_padded, training_label_seq, validation_padded, validation_label_seq = prepare_dataset_testing()\n",
        "\n",
        "    model = build_model()\n",
        "\n",
        "    history = train(model=model, x_train=train_padded, y_train=training_label_seq,\n",
        "                    x_validation=validation_padded, y_validation=validation_label_seq,\n",
        "                    epochs=EPOCHS, verbose=1)\n",
        "    plot_history(history)\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "    print('\\nTest:\\nLoss: {}\\nAccuracy: {}').format(loss, accuracy * 100)\n",
        "\n",
        "    model.save(model.h5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Xg-eujUahT",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **9. __name__ == \"__main__\"**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBushApRdFKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}