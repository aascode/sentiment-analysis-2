{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_require_predictVAD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwungFV_VPt1",
        "outputId": "c93bc94e-24b2-4a19-f499-a1e5a51258be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STRpWUg9K0kC"
      },
      "source": [
        "import sys\n",
        "import logging\n",
        "from psutil import virtual_memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjsabzdiK5mU"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "ram_gb = virtual_memory().total / 1e9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iLwc5IdK7eP"
      },
      "source": [
        "tf_response = {\n",
        "    'error': None,\n",
        "    'TF version': '',\n",
        "    'COLAB': None,\n",
        "    'GPU': False,\n",
        "    'ram_gb': ''\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnV8c7YOK9ok",
        "outputId": "578e3fcb-7f81-4dcb-f523-f6dc9f7683d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "try:\n",
        "    # drive\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "    # updating tensorflow version\n",
        "    %tensorflow_version 2.x\n",
        "\n",
        "    # tensorflow-gpu & tensorflowjs\n",
        "    !pip install tensorflow-gpu # !pip install tensorflow_text # I could use BERT\n",
        "    !pip install tensorflowjs\n",
        "\n",
        "    # NLP (nltk, stanza, spacy)\n",
        "    !pip install nltk \n",
        "    !pip install stanza\n",
        "    !pip install spacy\n",
        "    !spacy download en_core_web_sm # sm md lg\n",
        "    !python -m spacy download en\n",
        "except OSError as error:\n",
        "    # debugging error\n",
        "    response['error'] = logging.debug('You are not using your specify version of TensorFlow')\n",
        "    IN_COLAB = False\n",
        "\n",
        "    # install requirements\n",
        "    !pip install -r '../requirements.txt'\n",
        "finally:\n",
        "    tf_response['COLAB'] = IN_COLAB\n",
        "    \n",
        "    # Importing tensroflow core\n",
        "    import tensorflow as tf\n",
        "    import tensorflowjs as tfjs\n",
        "    from tensorflow import keras\n",
        "    from keras.utils import to_categorical\n",
        "    from sklearn import preprocessing\n",
        "\n",
        "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    from keras.callbacks import EarlyStopping\n",
        "    from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional, GlobalMaxPool1D\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    \n",
        "    # GPU and RAM response\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        GPU = True\n",
        "        tf_response['GPU'] = GPU\n",
        "        tf_response['TF_version'] = tf.__version__\n",
        "        \n",
        "        if tf_response['COLAB'] == True:\n",
        "            if gpu_info.find('failed') >= 0:\n",
        "                print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator')\n",
        "                print('Re-execute this cell.')\n",
        "            else:\n",
        "                print(gpu_info)\n",
        "            \n",
        "            if ram_gb < 20:\n",
        "                print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type menu\"')\n",
        "                print('Select high-RAM in the runtime shape dropdown')\n",
        "                print('Re-execute this cell')\n",
        "                tf_response['ram_gb'] = 'low-RAM runtime'\n",
        "            else:\n",
        "                tf_response['ram_gb'] = 'high-RAM runtime'\n",
        "            print('\\nRuntime {:.2f} GB of available RAM\\n'.format(ram_gb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.32.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.35.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow-gpu) (50.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: tensorflow-hub==0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (0.7.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.18.5)\n",
            "Requirement already satisfied: h5py>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.10.0)\n",
            "Requirement already satisfied: PyInquirer==1.0.3 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.0.3)\n",
            "Requirement already satisfied: tensorflow-cpu<3,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub==0.7.0->tensorflowjs) (3.12.4)\n",
            "Requirement already satisfied: Pygments>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from PyInquirer==1.0.3->tensorflowjs) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit==1.0.14 in /usr/local/lib/python3.6/dist-packages (from PyInquirer==1.0.3->tensorflowjs) (1.0.14)\n",
            "Requirement already satisfied: regex>=2016.11.21 in /usr/local/lib/python3.6/dist-packages (from PyInquirer==1.0.3->tensorflowjs) (2019.12.20)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.32.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub==0.7.0->tensorflowjs) (50.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit==1.0.14->PyInquirer==1.0.3->tensorflowjs) (0.2.5)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2.10)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.6.0+cu101)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (50.3.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnWGY06DLBMc",
        "outputId": "17674219-c212-4d42-9c61-41c55a6cd7ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf_response"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'COLAB': True, 'GPU': False, 'TF version': '', 'error': None, 'ram_gb': ''}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h-fSkWtLEqw"
      },
      "source": [
        "# Data analysis\n",
        "from keras.layers.merge import Concatenate\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.utils import plot_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import re\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator # to create a Word Cloud\n",
        "from PIL import Image # Pillow with WordCloud to image manipulation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JExjFvZXLHMl",
        "outputId": "9a725be1-29b2-49df-dd37-e4d20678da88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxW4v9BzLQvp"
      },
      "source": [
        "# Spacy NLP\n",
        "import spacy\n",
        "spNLP = spacy.load('en_core_web_sm')\n",
        "spNLP.max_length = 103950039 # or higher\n",
        "# spacy.prefer_gpu() #will not work witczh stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL9a6-hzLh9v"
      },
      "source": [
        "def nltk_lemma(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatizer.lemmatize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "141QBg64dLuI"
      },
      "source": [
        "## **Global Variable**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt5xeEaudPeK"
      },
      "source": [
        "maxlen = 30\n",
        "epochs = 30\n",
        "batch_size=16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P8qLOiUV820"
      },
      "source": [
        "def load_clean_dataset():\n",
        "  \n",
        "    !mkdir -p datasets\n",
        "    !wget -nc https://raw.githubusercontent.com/Y4rd13/sentiment-analysis/master/datasets/results/categories_dataset.csv -P datasets\n",
        "    df_cat = pd.read_csv('./datasets/categories_dataset.csv', encoding='utf-8')\n",
        "\n",
        "    df_cat.drop_duplicates(inplace=True)\n",
        "    df_cat = df_cat.loc[:, ~df_cat.columns.str.contains('^Unnamed')]\n",
        "\n",
        "    df = df_cat[['word','Valence','Arousal','Dominance','category']]\n",
        "    df.dropna(inplace=True)\n",
        "    \n",
        "    y = df['category']\n",
        "    X  = df[['word','Valence','Arousal','Dominance']]\n",
        "\n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G8b0ouNW5Vn"
      },
      "source": [
        "def preprocessing(X, y, maxlen):\n",
        "  \n",
        "  from sklearn import preprocessing\n",
        "  # The X variable contains the feature set, where as the y variable contains label set.\n",
        "\n",
        "  # label_encoder object knows how to understand word labels.\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "  # Encode labels in column 'category'.\n",
        "  y = label_encoder.fit_transform(y)\n",
        "\n",
        "  #splitting the dataset into train and test\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "  #convert to categorical variable\n",
        "  y_test_labels = y_test\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "\n",
        "  #only text data\n",
        "  X1_train =  X_train['word']\n",
        "  X1_test =  X_test['word']\n",
        "  \n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(X1_train)\n",
        "\n",
        "  X1_train = tokenizer.texts_to_sequences(X1_train)\n",
        "  X1_test = tokenizer.texts_to_sequences(X1_test)\n",
        "\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "  X1_train = pad_sequences(X1_train, padding='post', maxlen=maxlen)\n",
        "  X1_test = pad_sequences(X1_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "  #now we are taking numerical value\n",
        "\n",
        "  X2_train = X_train[['Valence','Arousal','Dominance']].values\n",
        "  X2_test = X_test[['Valence','Arousal','Dominance']].values\n",
        "\n",
        "  return X1_train,y_train,y_test, X1_test,X2_train,X2_test,maxlen,vocab_size,tokenizer, y_test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj1pr5TCYZ29"
      },
      "source": [
        "GloVe word embeddings for creating word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-pl2IYv2yf1"
      },
      "source": [
        "def GloVe(vocab_size,tokenizer):\n",
        "\n",
        "  embeddings_dictionary = dict()\n",
        "\n",
        "  glove_file = open('/content/drive/My Drive/glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "  for line in glove_file:\n",
        "      records = line.split()\n",
        "      word = records[0]\n",
        "      vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "      embeddings_dictionary[word] = vector_dimensions\n",
        "\n",
        "  glove_file.close()\n",
        "\n",
        "  embedding_matrix = zeros((vocab_size, 100))\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      embedding_vector = embeddings_dictionary.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          embedding_matrix[index] = embedding_vector\n",
        "  \n",
        "  return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Evzj4Z7ZNwH"
      },
      "source": [
        "def build_model(embedding_matrix,maxlen,vocab_size):\n",
        "\n",
        "  input_1 = Input(shape=(maxlen,)) #for text, The first submodel will accept textual input\n",
        "  input_2 = Input(shape=(3,)) #for number\n",
        "\n",
        "  embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(input_1)\n",
        "  LSTM_Layer_1 = LSTM(128)(embedding_layer) #This submodel will consist of an input shape layer, an embedding layer, and an LSTM layer of 128 neurons. \n",
        "\n",
        "  dense_layer_1 = Dense(10, activation='relu')(input_2)\n",
        "  dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)\n",
        "\n",
        "  concat_layer = Concatenate()([LSTM_Layer_1, dense_layer_2])\n",
        "  dense_layer_3 = Dense(10, activation='relu')(concat_layer)\n",
        "  output = Dense(10, activation='softmax')(dense_layer_3)\n",
        "  model = Model(inputs=[input_1, input_2], outputs=output)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=['acc'])\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvZdKvN-rte7"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNeGyLTQrEGf"
      },
      "source": [
        "def train(model,X1_train,X2_train,y_train,batch_size,epochs):\n",
        "\n",
        "  es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode = 'min', patience=5)\n",
        "  mc = tf.keras.callbacks.ModelCheckpoint('tfjsmodel.h5', monitor='val_acc',mode = 'max', verbose=1, save_best_only=True)\n",
        "\n",
        "  history = model.fit(x=[X1_train, X2_train], y=y_train, batch_size=batch_size, callbacks = [mc, es],epochs=epochs, verbose=1, validation_split=0.15)\n",
        "\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHxhZE6Cr48x"
      },
      "source": [
        "### **Plotting history**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08U7I08esJTZ"
      },
      "source": [
        "\n",
        "def plot_history_(history,score):\n",
        "\n",
        "  print(\"Test Score:\", score[0])\n",
        "  print(\"Test Accuracy:\", score[1])\n",
        "\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train','test'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.savefig('Accuracy.png')\n",
        "\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train','test'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.savefig('Loss.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdC4phGGapoK"
      },
      "source": [
        "## **Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK-uZkquahkn",
        "outputId": "c1bcade5-0710-40c9-d2ad-d3e3d1e7069d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X, y = load_clean_dataset() #load and clean dataset\n",
        "\n",
        "X1_train,y_train,y_test, X1_test,X2_train,X2_test,maxlen,vocab_size,tokenizer, y_test_labels = preprocessing(X, y, maxlen) #preprocess dataset\n",
        "\n",
        "embedding_matrix = GloVe(vocab_size, tokenizer) #glove\n",
        "\n",
        "model = build_model(embedding_matrix, maxlen,vocab_size) #building model\n",
        "\n",
        "plot_model(model, to_file='model_plot3.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "history = train(model,X1_train,X2_train,y_train, batch_size,epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘datasets/categories_dataset.csv’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  if self.run_code(code, result):\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 30, 100)      1998900     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           40          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 128)          117248      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           110         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 138)          0           lstm[0][0]                       \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           1390        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           110         dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,117,798\n",
            "Trainable params: 118,898\n",
            "Non-trainable params: 1,998,900\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "4055/4056 [============================>.] - ETA: 0s - loss: 1.7136 - acc: 0.3760\n",
            "Epoch 00001: val_acc improved from -inf to 0.48546, saving model to tfjsmodel.h5\n",
            "4056/4056 [==============================] - 73s 18ms/step - loss: 1.7136 - acc: 0.3760 - val_loss: 1.4020 - val_acc: 0.4855\n",
            "Epoch 2/30\n",
            "4053/4056 [============================>.] - ETA: 0s - loss: 1.2771 - acc: 0.5074\n",
            "Epoch 00002: val_acc improved from 0.48546 to 0.50607, saving model to tfjsmodel.h5\n",
            "4056/4056 [==============================] - 73s 18ms/step - loss: 1.2771 - acc: 0.5074 - val_loss: 1.1775 - val_acc: 0.5061\n",
            "Epoch 3/30\n",
            "4056/4056 [==============================] - ETA: 0s - loss: 1.1032 - acc: 0.5674\n",
            "Epoch 00003: val_acc improved from 0.50607 to 0.61968, saving model to tfjsmodel.h5\n",
            "4056/4056 [==============================] - 75s 19ms/step - loss: 1.1032 - acc: 0.5674 - val_loss: 1.0401 - val_acc: 0.6197\n",
            "Epoch 4/30\n",
            "4056/4056 [==============================] - ETA: 0s - loss: 0.9793 - acc: 0.6278\n",
            "Epoch 00004: val_acc improved from 0.61968 to 0.62754, saving model to tfjsmodel.h5\n",
            "4056/4056 [==============================] - 76s 19ms/step - loss: 0.9793 - acc: 0.6278 - val_loss: 0.9293 - val_acc: 0.6275\n",
            "Epoch 5/30\n",
            "4053/4056 [============================>.] - ETA: 0s - loss: 0.8833 - acc: 0.6329\n",
            "Epoch 00005: val_acc improved from 0.62754 to 0.63208, saving model to tfjsmodel.h5\n",
            "4056/4056 [==============================] - 74s 18ms/step - loss: 0.8832 - acc: 0.6329 - val_loss: 0.8463 - val_acc: 0.6321\n",
            "Epoch 6/30\n",
            "4055/4056 [============================>.] - ETA: 0s - loss: 0.8113 - acc: 0.6376\n",
            "Epoch 00006: val_acc did not improve from 0.63208\n",
            "4056/4056 [==============================] - 73s 18ms/step - loss: 0.8113 - acc: 0.6376 - val_loss: 0.7844 - val_acc: 0.6314\n",
            "Epoch 7/30\n",
            "4053/4056 [============================>.] - ETA: 0s - loss: 0.7532 - acc: 0.6437\n",
            "Epoch 00007: val_acc improved from 0.63208 to 0.64649, saving model to tfjsmodel.h5\n",
            "4056/4056 [==============================] - 74s 18ms/step - loss: 0.7531 - acc: 0.6437 - val_loss: 0.7318 - val_acc: 0.6465\n",
            "Epoch 8/30\n",
            "4055/4056 [============================>.] - ETA: 0s - loss: 0.7067 - acc: 0.6566\n",
            "Epoch 00008: val_acc improved from 0.64649 to 0.65986, saving model to tfjsmodel.h5\n",
            "4056/4056 [==============================] - 73s 18ms/step - loss: 0.7067 - acc: 0.6566 - val_loss: 0.6932 - val_acc: 0.6599\n",
            "Epoch 9/30\n",
            "4053/4056 [============================>.] - ETA: 0s - loss: 0.6695 - acc: 0.6679\n",
            "Epoch 00009: val_acc improved from 0.65986 to 0.66440, saving model to tfjsmodel.h5\n",
            "4056/4056 [==============================] - 75s 18ms/step - loss: 0.6694 - acc: 0.6680 - val_loss: 0.6595 - val_acc: 0.6644\n",
            "Epoch 10/30\n",
            "4056/4056 [==============================] - ETA: 0s - loss: 0.6452 - acc: 0.6735\n",
            "Epoch 00010: val_acc improved from 0.66440 to 0.66693, saving model to tfjsmodel.h5\n",
            "4056/4056 [==============================] - 79s 19ms/step - loss: 0.6452 - acc: 0.6735 - val_loss: 0.6400 - val_acc: 0.6669\n",
            "Epoch 11/30\n",
            "4053/4056 [============================>.] - ETA: 0s - loss: 0.6284 - acc: 0.6742\n",
            "Epoch 00011: val_acc improved from 0.66693 to 0.66999, saving model to tfjsmodel.h5\n",
            "4056/4056 [==============================] - 76s 19ms/step - loss: 0.6284 - acc: 0.6743 - val_loss: 0.6261 - val_acc: 0.6700\n",
            "Epoch 12/30\n",
            "4054/4056 [============================>.] - ETA: 0s - loss: 0.6164 - acc: 0.6759\n",
            "Epoch 00012: val_acc improved from 0.66999 to 0.67557, saving model to tfjsmodel.h5\n",
            "4056/4056 [==============================] - 75s 19ms/step - loss: 0.6164 - acc: 0.6759 - val_loss: 0.6159 - val_acc: 0.6756\n",
            "Epoch 13/30\n",
            "4053/4056 [============================>.] - ETA: 0s - loss: 0.6067 - acc: 0.6755\n",
            "Epoch 00013: val_acc did not improve from 0.67557\n",
            "4056/4056 [==============================] - 76s 19ms/step - loss: 0.6067 - acc: 0.6755 - val_loss: 0.6118 - val_acc: 0.6737\n",
            "Epoch 14/30\n",
            "4055/4056 [============================>.] - ETA: 0s - loss: 0.5985 - acc: 0.6769\n",
            "Epoch 00014: val_acc did not improve from 0.67557\n",
            "4056/4056 [==============================] - 75s 19ms/step - loss: 0.5985 - acc: 0.6769 - val_loss: 0.6003 - val_acc: 0.6719\n",
            "Epoch 15/30\n",
            "4055/4056 [============================>.] - ETA: 0s - loss: 0.5915 - acc: 0.6779\n",
            "Epoch 00015: val_acc did not improve from 0.67557\n",
            "4056/4056 [==============================] - 83s 21ms/step - loss: 0.5915 - acc: 0.6779 - val_loss: 0.5949 - val_acc: 0.6744\n",
            "Epoch 16/30\n",
            "4053/4056 [============================>.] - ETA: 0s - loss: 0.5858 - acc: 0.6783\n",
            "Epoch 00016: val_acc improved from 0.67557 to 0.67784, saving model to tfjsmodel.h5\n",
            "4056/4056 [==============================] - 80s 20ms/step - loss: 0.5858 - acc: 0.6783 - val_loss: 0.5887 - val_acc: 0.6778\n",
            "Epoch 17/30\n",
            "4055/4056 [============================>.] - ETA: 0s - loss: 0.5796 - acc: 0.6779\n",
            "Epoch 00017: val_acc did not improve from 0.67784\n",
            "4056/4056 [==============================] - 76s 19ms/step - loss: 0.5796 - acc: 0.6779 - val_loss: 0.5821 - val_acc: 0.6743\n",
            "Epoch 18/30\n",
            "4054/4056 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.6791\n",
            "Epoch 00018: val_acc did not improve from 0.67784\n",
            "4056/4056 [==============================] - 76s 19ms/step - loss: 0.5725 - acc: 0.6791 - val_loss: 0.5742 - val_acc: 0.6746\n",
            "Epoch 19/30\n",
            "4055/4056 [============================>.] - ETA: 0s - loss: 0.5647 - acc: 0.6803\n",
            "Epoch 00019: val_acc did not improve from 0.67784\n",
            "4056/4056 [==============================] - 76s 19ms/step - loss: 0.5647 - acc: 0.6803 - val_loss: 0.5760 - val_acc: 0.6725\n",
            "Epoch 20/30\n",
            "4056/4056 [==============================] - ETA: 0s - loss: 0.5564 - acc: 0.6796\n",
            "Epoch 00020: val_acc did not improve from 0.67784\n",
            "4056/4056 [==============================] - 77s 19ms/step - loss: 0.5564 - acc: 0.6796 - val_loss: 0.5615 - val_acc: 0.6739\n",
            "Epoch 21/30\n",
            "4056/4056 [==============================] - ETA: 0s - loss: 0.5474 - acc: 0.6806\n",
            "Epoch 00021: val_acc did not improve from 0.67784\n",
            "4056/4056 [==============================] - 78s 19ms/step - loss: 0.5474 - acc: 0.6806 - val_loss: 0.5525 - val_acc: 0.6753\n",
            "Epoch 22/30\n",
            "4055/4056 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.6804\n",
            "Epoch 00022: val_acc did not improve from 0.67784\n",
            "4056/4056 [==============================] - 77s 19ms/step - loss: 0.5387 - acc: 0.6804 - val_loss: 0.5428 - val_acc: 0.6685\n",
            "Epoch 23/30\n",
            "4055/4056 [============================>.] - ETA: 0s - loss: 0.5310 - acc: 0.6804\n",
            "Epoch 00023: val_acc did not improve from 0.67784\n",
            "4056/4056 [==============================] - 77s 19ms/step - loss: 0.5310 - acc: 0.6804 - val_loss: 0.5329 - val_acc: 0.6749\n",
            "Epoch 24/30\n",
            "4056/4056 [==============================] - ETA: 0s - loss: 0.5238 - acc: 0.6848\n",
            "Epoch 00024: val_acc improved from 0.67784 to 0.68186, saving model to tfjsmodel.h5\n",
            "4056/4056 [==============================] - 77s 19ms/step - loss: 0.5238 - acc: 0.6848 - val_loss: 0.5327 - val_acc: 0.6819\n",
            "Epoch 25/30\n",
            "4054/4056 [============================>.] - ETA: 0s - loss: 0.5185 - acc: 0.6821\n",
            "Epoch 00025: val_acc did not improve from 0.68186\n",
            "4056/4056 [==============================] - 75s 18ms/step - loss: 0.5185 - acc: 0.6821 - val_loss: 0.5266 - val_acc: 0.6792\n",
            "Epoch 26/30\n",
            "4053/4056 [============================>.] - ETA: 0s - loss: 0.5146 - acc: 0.6812\n",
            "Epoch 00026: val_acc did not improve from 0.68186\n",
            "4056/4056 [==============================] - 77s 19ms/step - loss: 0.5146 - acc: 0.6811 - val_loss: 0.5212 - val_acc: 0.6754\n",
            "Epoch 27/30\n",
            "4054/4056 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.6834\n",
            "Epoch 00027: val_acc did not improve from 0.68186\n",
            "4056/4056 [==============================] - 79s 20ms/step - loss: 0.5112 - acc: 0.6834 - val_loss: 0.5155 - val_acc: 0.6736\n",
            "Epoch 28/30\n",
            "4055/4056 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.6851\n",
            "Epoch 00028: val_acc did not improve from 0.68186\n",
            "4056/4056 [==============================] - 77s 19ms/step - loss: 0.5083 - acc: 0.6851 - val_loss: 0.5152 - val_acc: 0.6715\n",
            "Epoch 29/30\n",
            "4054/4056 [============================>.] - ETA: 0s - loss: 0.5059 - acc: 0.6888\n",
            "Epoch 00029: val_acc did not improve from 0.68186\n",
            "4056/4056 [==============================] - 77s 19ms/step - loss: 0.5058 - acc: 0.6888 - val_loss: 0.5127 - val_acc: 0.6751\n",
            "Epoch 30/30\n",
            "4054/4056 [============================>.] - ETA: 0s - loss: 0.5049 - acc: 0.6853\n",
            "Epoch 00030: val_acc did not improve from 0.68186\n",
            "4056/4056 [==============================] - 77s 19ms/step - loss: 0.5049 - acc: 0.6854 - val_loss: 0.5109 - val_acc: 0.6710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcZ3cp7nK9Un"
      },
      "source": [
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzqwdfxeGBP0",
        "outputId": "54093aa7-9560-487e-9330-b1855b71e8dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_model = load_model('tfjsmodel.h5')\n",
        "prediction = test_model.predict(x=[X1_test,X2_test], verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "421/421 [==============================] - 10s 24ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lvsq3t-QnNgz",
        "outputId": "2abdd3dc-8c8a-4206-d824-ea05395e0eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 2.5040624e-01,\n",
              "        1.4474839e-15, 3.3088673e-10],\n",
              "       [1.5616773e-06, 3.1854683e-03, 7.9136451e-05, ..., 4.1300807e-09,\n",
              "        2.0041482e-09, 5.0861394e-04],\n",
              "       [5.2709973e-01, 4.5314166e-01, 4.5239049e-04, ..., 6.5060147e-14,\n",
              "        1.5599744e-12, 1.1741150e-08],\n",
              "       ...,\n",
              "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 2.1620320e-02,\n",
              "        3.2118027e-18, 1.3074522e-10],\n",
              "       [1.2559842e-06, 2.5010994e-03, 5.9200596e-05, ..., 3.2649863e-09,\n",
              "        2.1630269e-09, 4.2986666e-04],\n",
              "       [4.6946737e-01, 4.2423484e-01, 1.3772710e-04, ..., 1.1460935e-14,\n",
              "        3.8080537e-12, 3.5124068e-09]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8pV6rsdoDBe"
      },
      "source": [
        "y_classes = prediction.argmax(axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGpC7hLGoS-0",
        "outputId": "667a2e2b-3b39-4a4b-8279-df61e6feb5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_classes.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13472,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvYlcPVqoZde",
        "outputId": "b5c11d1c-12b1-4a2e-909a-e7d90ac1efa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "421/421 [==============================] - 6s 13ms/step - loss: 0.5099 - acc: 0.6644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUFFpM21rwdw",
        "outputId": "d4cfbd00-2fe8-42d4-800b-969e5692cf75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13472,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_eU3tfGr2uC"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(data = prediction[0:, 0:], \n",
        "                  index = [str(i) for i in y_test_labels],\n",
        "                  columns = [str(i) for i in range(prediction.shape[1])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InSI2ZR0sU1R",
        "outputId": "9fae3737-b35c-4cb9-d61d-dc0f0cf690fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.814949e-34</td>\n",
              "      <td>5.272422e-01</td>\n",
              "      <td>1.872099e-21</td>\n",
              "      <td>2.223515e-01</td>\n",
              "      <td>2.504062e-01</td>\n",
              "      <td>1.447484e-15</td>\n",
              "      <td>3.308867e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.561677e-06</td>\n",
              "      <td>3.185468e-03</td>\n",
              "      <td>7.913645e-05</td>\n",
              "      <td>9.777549e-01</td>\n",
              "      <td>8.974229e-09</td>\n",
              "      <td>1.847034e-02</td>\n",
              "      <td>1.210822e-13</td>\n",
              "      <td>4.130081e-09</td>\n",
              "      <td>2.004148e-09</td>\n",
              "      <td>5.086139e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.270997e-01</td>\n",
              "      <td>4.531417e-01</td>\n",
              "      <td>4.523905e-04</td>\n",
              "      <td>5.446275e-04</td>\n",
              "      <td>1.713505e-13</td>\n",
              "      <td>1.876152e-02</td>\n",
              "      <td>2.591948e-19</td>\n",
              "      <td>6.506015e-14</td>\n",
              "      <td>1.559974e-12</td>\n",
              "      <td>1.174115e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.429084e-27</td>\n",
              "      <td>6.693740e-23</td>\n",
              "      <td>5.168884e-26</td>\n",
              "      <td>9.710953e-12</td>\n",
              "      <td>9.553616e-01</td>\n",
              "      <td>7.918949e-05</td>\n",
              "      <td>9.747206e-04</td>\n",
              "      <td>4.112374e-02</td>\n",
              "      <td>3.637110e-06</td>\n",
              "      <td>2.457073e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.378620e-31</td>\n",
              "      <td>5.579849e-01</td>\n",
              "      <td>4.962835e-18</td>\n",
              "      <td>1.076320e-01</td>\n",
              "      <td>3.343830e-01</td>\n",
              "      <td>2.123751e-11</td>\n",
              "      <td>1.936470e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.905152e-34</td>\n",
              "      <td>3.784591e-01</td>\n",
              "      <td>3.634475e-22</td>\n",
              "      <td>3.138541e-01</td>\n",
              "      <td>3.076868e-01</td>\n",
              "      <td>5.721536e-16</td>\n",
              "      <td>3.414659e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.432897e-31</td>\n",
              "      <td>5.380160e-01</td>\n",
              "      <td>3.673253e-14</td>\n",
              "      <td>2.336550e-02</td>\n",
              "      <td>4.385383e-01</td>\n",
              "      <td>8.029237e-05</td>\n",
              "      <td>1.821004e-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.739637e-36</td>\n",
              "      <td>9.410002e-01</td>\n",
              "      <td>7.464538e-24</td>\n",
              "      <td>3.737944e-02</td>\n",
              "      <td>2.162032e-02</td>\n",
              "      <td>3.211803e-18</td>\n",
              "      <td>1.307452e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.255984e-06</td>\n",
              "      <td>2.501099e-03</td>\n",
              "      <td>5.920060e-05</td>\n",
              "      <td>9.731141e-01</td>\n",
              "      <td>8.090341e-09</td>\n",
              "      <td>2.389454e-02</td>\n",
              "      <td>1.014432e-13</td>\n",
              "      <td>3.264986e-09</td>\n",
              "      <td>2.163027e-09</td>\n",
              "      <td>4.298667e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.694674e-01</td>\n",
              "      <td>4.242348e-01</td>\n",
              "      <td>1.377271e-04</td>\n",
              "      <td>3.636517e-04</td>\n",
              "      <td>5.869216e-14</td>\n",
              "      <td>1.057965e-01</td>\n",
              "      <td>4.849143e-20</td>\n",
              "      <td>1.146094e-14</td>\n",
              "      <td>3.808054e-12</td>\n",
              "      <td>3.512407e-09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13472 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               0             1  ...             8             9\n",
              "7   0.000000e+00  0.000000e+00  ...  1.447484e-15  3.308867e-10\n",
              "3   1.561677e-06  3.185468e-03  ...  2.004148e-09  5.086139e-04\n",
              "1   5.270997e-01  4.531417e-01  ...  1.559974e-12  1.174115e-08\n",
              "4   2.429084e-27  6.693740e-23  ...  3.637110e-06  2.457073e-03\n",
              "4   0.000000e+00  0.000000e+00  ...  2.123751e-11  1.936470e-10\n",
              "..           ...           ...  ...           ...           ...\n",
              "6   0.000000e+00  0.000000e+00  ...  5.721536e-16  3.414659e-10\n",
              "4   0.000000e+00  0.000000e+00  ...  8.029237e-05  1.821004e-12\n",
              "4   0.000000e+00  0.000000e+00  ...  3.211803e-18  1.307452e-10\n",
              "3   1.255984e-06  2.501099e-03  ...  2.163027e-09  4.298667e-04\n",
              "0   4.694674e-01  4.242348e-01  ...  3.808054e-12  3.512407e-09\n",
              "\n",
              "[13472 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6r-TUPgttxn",
        "outputId": "93146394-32a7-401a-e604-8d12eea4cbee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        }
      },
      "source": [
        "df = df.T\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>7</th>\n",
              "      <th>3</th>\n",
              "      <th>1</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>6</th>\n",
              "      <th>0</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>1</th>\n",
              "      <th>9</th>\n",
              "      <th>6</th>\n",
              "      <th>4</th>\n",
              "      <th>7</th>\n",
              "      <th>5</th>\n",
              "      <th>2</th>\n",
              "      <th>7</th>\n",
              "      <th>6</th>\n",
              "      <th>9</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>9</th>\n",
              "      <th>2</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>3</th>\n",
              "      <th>1</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>2</th>\n",
              "      <th>1</th>\n",
              "      <th>1</th>\n",
              "      <th>4</th>\n",
              "      <th>...</th>\n",
              "      <th>1</th>\n",
              "      <th>0</th>\n",
              "      <th>9</th>\n",
              "      <th>4</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>7</th>\n",
              "      <th>4</th>\n",
              "      <th>6</th>\n",
              "      <th>6</th>\n",
              "      <th>2</th>\n",
              "      <th>5</th>\n",
              "      <th>7</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>3</th>\n",
              "      <th>2</th>\n",
              "      <th>2</th>\n",
              "      <th>2</th>\n",
              "      <th>8</th>\n",
              "      <th>1</th>\n",
              "      <th>7</th>\n",
              "      <th>5</th>\n",
              "      <th>4</th>\n",
              "      <th>6</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>3</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.561677e-06</td>\n",
              "      <td>5.270997e-01</td>\n",
              "      <td>2.429084e-27</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.158160e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.735165e-05</td>\n",
              "      <td>3.668198e-17</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.776904e-09</td>\n",
              "      <td>3.651611e-05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.144307e-17</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.576615e-17</td>\n",
              "      <td>3.637183e-05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.513411e-29</td>\n",
              "      <td>5.311974e-07</td>\n",
              "      <td>5.130428e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>8.653192e-37</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.363348e-05</td>\n",
              "      <td>5.380855e-01</td>\n",
              "      <td>4.314213e-05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>3.891398e-05</td>\n",
              "      <td>4.709960e-01</td>\n",
              "      <td>4.038349e-17</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.898001e-05</td>\n",
              "      <td>2.880606e-15</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.340457e-06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.340457e-06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.799407e-06</td>\n",
              "      <td>3.667599e-05</td>\n",
              "      <td>4.382597e-05</td>\n",
              "      <td>3.732224e-05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.277032e-05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.559290e-09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.255984e-06</td>\n",
              "      <td>4.694674e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.185468e-03</td>\n",
              "      <td>4.531417e-01</td>\n",
              "      <td>6.693740e-23</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.696563e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.757914e-01</td>\n",
              "      <td>1.169106e-09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.007520e-10</td>\n",
              "      <td>5.677760e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.416806e-10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.253052e-38</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.167662e-09</td>\n",
              "      <td>5.615577e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.028161e-24</td>\n",
              "      <td>1.126621e-03</td>\n",
              "      <td>4.704629e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.768751e-33</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.759759e-38</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.632722e-01</td>\n",
              "      <td>4.281384e-01</td>\n",
              "      <td>6.798029e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>5.615543e-01</td>\n",
              "      <td>4.243913e-01</td>\n",
              "      <td>1.240998e-09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.546896e-01</td>\n",
              "      <td>1.618275e-13</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.684672e-03</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.336187e-38</td>\n",
              "      <td>2.684672e-03</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.516747e-03</td>\n",
              "      <td>5.800095e-01</td>\n",
              "      <td>5.571595e-01</td>\n",
              "      <td>6.102787e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.612892e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.097847e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.501099e-03</td>\n",
              "      <td>4.242348e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>7.913645e-05</td>\n",
              "      <td>4.523905e-04</td>\n",
              "      <td>5.168884e-26</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6.053488e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.222555e-01</td>\n",
              "      <td>4.324693e-09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.145879e-15</td>\n",
              "      <td>4.305170e-01</td>\n",
              "      <td>4.592374e-38</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.857742e-09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>7.959877e-38</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.300118e-38</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.314297e-09</td>\n",
              "      <td>4.368617e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.149106e-27</td>\n",
              "      <td>1.700986e-05</td>\n",
              "      <td>5.614549e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.078333e-37</td>\n",
              "      <td>8.614149e-38</td>\n",
              "      <td>6.239887e-38</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.353651e-01</td>\n",
              "      <td>2.909722e-04</td>\n",
              "      <td>3.172506e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>4.370492e-01</td>\n",
              "      <td>1.385848e-04</td>\n",
              "      <td>4.814291e-09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.534098e-38</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.440430e-01</td>\n",
              "      <td>1.601549e-17</td>\n",
              "      <td>7.935331e-38</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6.431768e-05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.260890e-37</td>\n",
              "      <td>6.431768e-05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>8.780314e-05</td>\n",
              "      <td>4.177130e-01</td>\n",
              "      <td>4.415888e-01</td>\n",
              "      <td>3.868808e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.373572e-01</td>\n",
              "      <td>8.311837e-38</td>\n",
              "      <td>2.103068e-10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.920060e-05</td>\n",
              "      <td>1.377271e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.814949e-34</td>\n",
              "      <td>9.777549e-01</td>\n",
              "      <td>5.446275e-04</td>\n",
              "      <td>9.710953e-12</td>\n",
              "      <td>1.378620e-31</td>\n",
              "      <td>2.456891e-34</td>\n",
              "      <td>5.679999e-04</td>\n",
              "      <td>1.221200e-34</td>\n",
              "      <td>1.253603e-30</td>\n",
              "      <td>1.865577e-03</td>\n",
              "      <td>2.302184e-04</td>\n",
              "      <td>1.164473e-34</td>\n",
              "      <td>1.550465e-34</td>\n",
              "      <td>2.201292e-34</td>\n",
              "      <td>1.337115e-05</td>\n",
              "      <td>1.624050e-03</td>\n",
              "      <td>5.513233e-24</td>\n",
              "      <td>1.924473e-34</td>\n",
              "      <td>2.608823e-04</td>\n",
              "      <td>8.395438e-31</td>\n",
              "      <td>2.091467e-23</td>\n",
              "      <td>2.388064e-34</td>\n",
              "      <td>7.948469e-24</td>\n",
              "      <td>1.819328e-34</td>\n",
              "      <td>2.338389e-04</td>\n",
              "      <td>1.499449e-03</td>\n",
              "      <td>5.534067e-36</td>\n",
              "      <td>4.780757e-13</td>\n",
              "      <td>8.983201e-01</td>\n",
              "      <td>5.565775e-04</td>\n",
              "      <td>3.395728e-26</td>\n",
              "      <td>9.561129e-32</td>\n",
              "      <td>7.087181e-19</td>\n",
              "      <td>5.906288e-24</td>\n",
              "      <td>1.444868e-23</td>\n",
              "      <td>1.557755e-34</td>\n",
              "      <td>1.273835e-03</td>\n",
              "      <td>5.138168e-04</td>\n",
              "      <td>2.859377e-03</td>\n",
              "      <td>1.670063e-36</td>\n",
              "      <td>...</td>\n",
              "      <td>1.312797e-03</td>\n",
              "      <td>3.646803e-04</td>\n",
              "      <td>2.099486e-04</td>\n",
              "      <td>1.670724e-34</td>\n",
              "      <td>8.394828e-24</td>\n",
              "      <td>3.187096e-31</td>\n",
              "      <td>3.393590e-34</td>\n",
              "      <td>2.678049e-34</td>\n",
              "      <td>1.720542e-34</td>\n",
              "      <td>2.267634e-34</td>\n",
              "      <td>1.185494e-03</td>\n",
              "      <td>3.934209e-06</td>\n",
              "      <td>9.142500e-24</td>\n",
              "      <td>2.419127e-34</td>\n",
              "      <td>3.599812e-36</td>\n",
              "      <td>9.743557e-01</td>\n",
              "      <td>2.100291e-30</td>\n",
              "      <td>1.554236e-34</td>\n",
              "      <td>1.232289e-34</td>\n",
              "      <td>1.023740e-23</td>\n",
              "      <td>9.743557e-01</td>\n",
              "      <td>2.262270e-36</td>\n",
              "      <td>1.474122e-34</td>\n",
              "      <td>2.631221e-36</td>\n",
              "      <td>1.240192e-34</td>\n",
              "      <td>1.229155e-31</td>\n",
              "      <td>9.785606e-01</td>\n",
              "      <td>2.189403e-03</td>\n",
              "      <td>1.164251e-03</td>\n",
              "      <td>2.746713e-03</td>\n",
              "      <td>2.202905e-31</td>\n",
              "      <td>1.265832e-03</td>\n",
              "      <td>9.859180e-24</td>\n",
              "      <td>1.205448e-02</td>\n",
              "      <td>2.570287e-28</td>\n",
              "      <td>1.905152e-34</td>\n",
              "      <td>9.432897e-31</td>\n",
              "      <td>4.739637e-36</td>\n",
              "      <td>9.731141e-01</td>\n",
              "      <td>3.636517e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.272422e-01</td>\n",
              "      <td>8.974229e-09</td>\n",
              "      <td>1.713505e-13</td>\n",
              "      <td>9.553616e-01</td>\n",
              "      <td>5.579849e-01</td>\n",
              "      <td>4.634473e-01</td>\n",
              "      <td>2.298062e-13</td>\n",
              "      <td>3.953276e-01</td>\n",
              "      <td>5.024408e-01</td>\n",
              "      <td>3.791917e-12</td>\n",
              "      <td>3.422962e-05</td>\n",
              "      <td>3.491937e-01</td>\n",
              "      <td>3.897116e-01</td>\n",
              "      <td>4.941903e-01</td>\n",
              "      <td>1.412248e-11</td>\n",
              "      <td>3.296475e-12</td>\n",
              "      <td>4.651750e-02</td>\n",
              "      <td>3.985982e-01</td>\n",
              "      <td>3.934071e-05</td>\n",
              "      <td>5.870717e-01</td>\n",
              "      <td>6.248686e-02</td>\n",
              "      <td>5.514408e-01</td>\n",
              "      <td>3.844493e-02</td>\n",
              "      <td>4.507132e-01</td>\n",
              "      <td>3.376515e-05</td>\n",
              "      <td>3.069468e-12</td>\n",
              "      <td>9.612908e-01</td>\n",
              "      <td>9.434873e-01</td>\n",
              "      <td>3.737376e-09</td>\n",
              "      <td>2.199694e-13</td>\n",
              "      <td>8.400991e-01</td>\n",
              "      <td>1.342584e-01</td>\n",
              "      <td>9.710906e-01</td>\n",
              "      <td>3.539244e-02</td>\n",
              "      <td>7.806276e-02</td>\n",
              "      <td>3.989224e-01</td>\n",
              "      <td>3.715277e-12</td>\n",
              "      <td>1.190317e-13</td>\n",
              "      <td>3.317376e-12</td>\n",
              "      <td>8.725960e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>3.286225e-12</td>\n",
              "      <td>5.843517e-14</td>\n",
              "      <td>3.457355e-05</td>\n",
              "      <td>3.821282e-01</td>\n",
              "      <td>3.889597e-02</td>\n",
              "      <td>2.548176e-05</td>\n",
              "      <td>6.215332e-01</td>\n",
              "      <td>4.155816e-01</td>\n",
              "      <td>4.301901e-01</td>\n",
              "      <td>4.093728e-01</td>\n",
              "      <td>3.004703e-12</td>\n",
              "      <td>1.213343e-04</td>\n",
              "      <td>4.382140e-02</td>\n",
              "      <td>5.330206e-01</td>\n",
              "      <td>8.995600e-01</td>\n",
              "      <td>8.498253e-09</td>\n",
              "      <td>5.348181e-01</td>\n",
              "      <td>3.594287e-01</td>\n",
              "      <td>4.063013e-01</td>\n",
              "      <td>3.678178e-02</td>\n",
              "      <td>8.498253e-09</td>\n",
              "      <td>8.836523e-01</td>\n",
              "      <td>3.776605e-01</td>\n",
              "      <td>8.774734e-01</td>\n",
              "      <td>3.697467e-01</td>\n",
              "      <td>1.827792e-05</td>\n",
              "      <td>1.021984e-08</td>\n",
              "      <td>3.682232e-12</td>\n",
              "      <td>3.462605e-12</td>\n",
              "      <td>4.346634e-12</td>\n",
              "      <td>3.676633e-05</td>\n",
              "      <td>3.619010e-12</td>\n",
              "      <td>4.450407e-02</td>\n",
              "      <td>9.075376e-07</td>\n",
              "      <td>1.409400e-01</td>\n",
              "      <td>3.784591e-01</td>\n",
              "      <td>5.380160e-01</td>\n",
              "      <td>9.410002e-01</td>\n",
              "      <td>8.090341e-09</td>\n",
              "      <td>5.869216e-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.872099e-21</td>\n",
              "      <td>1.847034e-02</td>\n",
              "      <td>1.876152e-02</td>\n",
              "      <td>7.918949e-05</td>\n",
              "      <td>4.962835e-18</td>\n",
              "      <td>8.731812e-22</td>\n",
              "      <td>1.335423e-02</td>\n",
              "      <td>8.411982e-22</td>\n",
              "      <td>2.081202e-14</td>\n",
              "      <td>6.190708e-07</td>\n",
              "      <td>1.119539e-07</td>\n",
              "      <td>3.508389e-22</td>\n",
              "      <td>5.150884e-22</td>\n",
              "      <td>1.808309e-21</td>\n",
              "      <td>9.999865e-01</td>\n",
              "      <td>4.686740e-07</td>\n",
              "      <td>1.829744e-19</td>\n",
              "      <td>6.280698e-22</td>\n",
              "      <td>3.142956e-07</td>\n",
              "      <td>1.575859e-13</td>\n",
              "      <td>8.256891e-19</td>\n",
              "      <td>5.429523e-21</td>\n",
              "      <td>9.711531e-20</td>\n",
              "      <td>9.301183e-22</td>\n",
              "      <td>1.169328e-07</td>\n",
              "      <td>3.855894e-07</td>\n",
              "      <td>9.216260e-24</td>\n",
              "      <td>5.592581e-06</td>\n",
              "      <td>1.003602e-01</td>\n",
              "      <td>1.537633e-02</td>\n",
              "      <td>3.235251e-13</td>\n",
              "      <td>2.926367e-23</td>\n",
              "      <td>6.121478e-07</td>\n",
              "      <td>6.337236e-20</td>\n",
              "      <td>3.171035e-18</td>\n",
              "      <td>4.028160e-22</td>\n",
              "      <td>2.675293e-07</td>\n",
              "      <td>3.297124e-02</td>\n",
              "      <td>3.784677e-06</td>\n",
              "      <td>7.987497e-24</td>\n",
              "      <td>...</td>\n",
              "      <td>2.745950e-07</td>\n",
              "      <td>1.041095e-01</td>\n",
              "      <td>7.974043e-08</td>\n",
              "      <td>3.688124e-22</td>\n",
              "      <td>1.038802e-19</td>\n",
              "      <td>5.017737e-12</td>\n",
              "      <td>4.394694e-21</td>\n",
              "      <td>2.814490e-22</td>\n",
              "      <td>8.583668e-22</td>\n",
              "      <td>3.760777e-22</td>\n",
              "      <td>2.062419e-07</td>\n",
              "      <td>9.997923e-01</td>\n",
              "      <td>1.694703e-19</td>\n",
              "      <td>3.323268e-21</td>\n",
              "      <td>6.304519e-24</td>\n",
              "      <td>2.243997e-02</td>\n",
              "      <td>6.952371e-14</td>\n",
              "      <td>3.557297e-22</td>\n",
              "      <td>1.068471e-21</td>\n",
              "      <td>8.889304e-20</td>\n",
              "      <td>2.243997e-02</td>\n",
              "      <td>8.917929e-24</td>\n",
              "      <td>5.083765e-22</td>\n",
              "      <td>4.840066e-24</td>\n",
              "      <td>4.263399e-22</td>\n",
              "      <td>5.416804e-12</td>\n",
              "      <td>1.728132e-02</td>\n",
              "      <td>8.250462e-07</td>\n",
              "      <td>2.075876e-07</td>\n",
              "      <td>1.488237e-06</td>\n",
              "      <td>5.479318e-12</td>\n",
              "      <td>2.583241e-07</td>\n",
              "      <td>1.897730e-19</td>\n",
              "      <td>9.878013e-01</td>\n",
              "      <td>7.069929e-21</td>\n",
              "      <td>3.634475e-22</td>\n",
              "      <td>3.673253e-14</td>\n",
              "      <td>7.464538e-24</td>\n",
              "      <td>2.389454e-02</td>\n",
              "      <td>1.057965e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.223515e-01</td>\n",
              "      <td>1.210822e-13</td>\n",
              "      <td>2.591948e-19</td>\n",
              "      <td>9.747206e-04</td>\n",
              "      <td>1.076320e-01</td>\n",
              "      <td>2.608072e-01</td>\n",
              "      <td>4.191213e-19</td>\n",
              "      <td>3.075534e-01</td>\n",
              "      <td>2.491493e-02</td>\n",
              "      <td>4.162070e-17</td>\n",
              "      <td>1.181841e-08</td>\n",
              "      <td>3.395344e-01</td>\n",
              "      <td>3.091098e-01</td>\n",
              "      <td>2.410136e-01</td>\n",
              "      <td>5.122648e-17</td>\n",
              "      <td>3.654693e-17</td>\n",
              "      <td>6.267740e-03</td>\n",
              "      <td>2.994881e-01</td>\n",
              "      <td>1.042511e-08</td>\n",
              "      <td>1.849684e-02</td>\n",
              "      <td>5.992752e-03</td>\n",
              "      <td>2.033421e-01</td>\n",
              "      <td>5.706829e-03</td>\n",
              "      <td>2.700500e-01</td>\n",
              "      <td>1.145600e-08</td>\n",
              "      <td>3.449665e-17</td>\n",
              "      <td>2.378825e-02</td>\n",
              "      <td>1.488260e-03</td>\n",
              "      <td>2.799516e-14</td>\n",
              "      <td>3.894214e-19</td>\n",
              "      <td>2.667348e-02</td>\n",
              "      <td>1.086229e-01</td>\n",
              "      <td>2.436784e-03</td>\n",
              "      <td>5.913272e-03</td>\n",
              "      <td>6.186669e-03</td>\n",
              "      <td>3.044059e-01</td>\n",
              "      <td>5.168262e-17</td>\n",
              "      <td>1.411544e-19</td>\n",
              "      <td>2.366262e-17</td>\n",
              "      <td>8.597966e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>4.182566e-17</td>\n",
              "      <td>4.816258e-20</td>\n",
              "      <td>1.338499e-08</td>\n",
              "      <td>3.127328e-01</td>\n",
              "      <td>5.677643e-03</td>\n",
              "      <td>8.641906e-08</td>\n",
              "      <td>1.707089e-01</td>\n",
              "      <td>2.898495e-01</td>\n",
              "      <td>2.824045e-01</td>\n",
              "      <td>2.943746e-01</td>\n",
              "      <td>3.899037e-17</td>\n",
              "      <td>5.924408e-09</td>\n",
              "      <td>5.731451e-03</td>\n",
              "      <td>2.180924e-01</td>\n",
              "      <td>6.571021e-02</td>\n",
              "      <td>1.099468e-13</td>\n",
              "      <td>1.900485e-02</td>\n",
              "      <td>3.276377e-01</td>\n",
              "      <td>2.975610e-01</td>\n",
              "      <td>5.430372e-03</td>\n",
              "      <td>1.099468e-13</td>\n",
              "      <td>7.738296e-02</td>\n",
              "      <td>3.152019e-01</td>\n",
              "      <td>8.214327e-02</td>\n",
              "      <td>3.247450e-01</td>\n",
              "      <td>5.813467e-08</td>\n",
              "      <td>1.490740e-13</td>\n",
              "      <td>3.600535e-17</td>\n",
              "      <td>4.924158e-17</td>\n",
              "      <td>3.969961e-17</td>\n",
              "      <td>1.364375e-07</td>\n",
              "      <td>4.986535e-17</td>\n",
              "      <td>5.654106e-03</td>\n",
              "      <td>1.713902e-11</td>\n",
              "      <td>4.033060e-02</td>\n",
              "      <td>3.138541e-01</td>\n",
              "      <td>2.336550e-02</td>\n",
              "      <td>3.737944e-02</td>\n",
              "      <td>1.014432e-13</td>\n",
              "      <td>4.849143e-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.504062e-01</td>\n",
              "      <td>4.130081e-09</td>\n",
              "      <td>6.506015e-14</td>\n",
              "      <td>4.112374e-02</td>\n",
              "      <td>3.343830e-01</td>\n",
              "      <td>2.757454e-01</td>\n",
              "      <td>1.010976e-13</td>\n",
              "      <td>2.971191e-01</td>\n",
              "      <td>4.725864e-01</td>\n",
              "      <td>1.414248e-10</td>\n",
              "      <td>6.239006e-04</td>\n",
              "      <td>3.112720e-01</td>\n",
              "      <td>3.011785e-01</td>\n",
              "      <td>2.647961e-01</td>\n",
              "      <td>1.655819e-14</td>\n",
              "      <td>1.318974e-10</td>\n",
              "      <td>9.471445e-01</td>\n",
              "      <td>3.019136e-01</td>\n",
              "      <td>5.508258e-04</td>\n",
              "      <td>3.941495e-01</td>\n",
              "      <td>9.313992e-01</td>\n",
              "      <td>2.452171e-01</td>\n",
              "      <td>9.557633e-01</td>\n",
              "      <td>2.792368e-01</td>\n",
              "      <td>6.092791e-04</td>\n",
              "      <td>1.289717e-10</td>\n",
              "      <td>1.492099e-02</td>\n",
              "      <td>5.371211e-02</td>\n",
              "      <td>8.054465e-10</td>\n",
              "      <td>9.212675e-14</td>\n",
              "      <td>1.332274e-01</td>\n",
              "      <td>7.571186e-01</td>\n",
              "      <td>2.639706e-02</td>\n",
              "      <td>9.586196e-01</td>\n",
              "      <td>9.156473e-01</td>\n",
              "      <td>2.966717e-01</td>\n",
              "      <td>1.687025e-10</td>\n",
              "      <td>3.576470e-14</td>\n",
              "      <td>7.564482e-11</td>\n",
              "      <td>4.142436e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>1.494284e-10</td>\n",
              "      <td>1.146770e-14</td>\n",
              "      <td>6.884790e-04</td>\n",
              "      <td>3.051390e-01</td>\n",
              "      <td>9.553391e-01</td>\n",
              "      <td>1.683381e-05</td>\n",
              "      <td>2.077579e-01</td>\n",
              "      <td>2.945690e-01</td>\n",
              "      <td>2.874054e-01</td>\n",
              "      <td>2.962525e-01</td>\n",
              "      <td>1.466742e-10</td>\n",
              "      <td>9.989210e-07</td>\n",
              "      <td>9.503579e-01</td>\n",
              "      <td>2.488869e-01</td>\n",
              "      <td>3.472976e-02</td>\n",
              "      <td>3.551698e-09</td>\n",
              "      <td>4.460338e-01</td>\n",
              "      <td>3.129336e-01</td>\n",
              "      <td>2.961376e-01</td>\n",
              "      <td>9.576917e-01</td>\n",
              "      <td>3.551698e-09</td>\n",
              "      <td>3.896474e-02</td>\n",
              "      <td>3.071377e-01</td>\n",
              "      <td>4.038337e-02</td>\n",
              "      <td>3.055082e-01</td>\n",
              "      <td>1.049154e-05</td>\n",
              "      <td>4.886654e-09</td>\n",
              "      <td>1.280709e-10</td>\n",
              "      <td>1.674152e-10</td>\n",
              "      <td>1.297584e-10</td>\n",
              "      <td>2.230820e-05</td>\n",
              "      <td>1.660363e-10</td>\n",
              "      <td>9.497484e-01</td>\n",
              "      <td>3.634709e-08</td>\n",
              "      <td>8.187279e-01</td>\n",
              "      <td>3.076868e-01</td>\n",
              "      <td>4.385383e-01</td>\n",
              "      <td>2.162032e-02</td>\n",
              "      <td>3.264986e-09</td>\n",
              "      <td>1.146094e-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.447484e-15</td>\n",
              "      <td>2.004148e-09</td>\n",
              "      <td>1.559974e-12</td>\n",
              "      <td>3.637110e-06</td>\n",
              "      <td>2.123751e-11</td>\n",
              "      <td>8.648796e-16</td>\n",
              "      <td>1.451616e-12</td>\n",
              "      <td>8.880208e-16</td>\n",
              "      <td>5.794607e-05</td>\n",
              "      <td>1.037145e-15</td>\n",
              "      <td>9.208795e-12</td>\n",
              "      <td>5.355674e-16</td>\n",
              "      <td>6.771091e-16</td>\n",
              "      <td>1.399907e-15</td>\n",
              "      <td>1.302818e-07</td>\n",
              "      <td>8.016184e-16</td>\n",
              "      <td>2.213030e-17</td>\n",
              "      <td>7.763465e-16</td>\n",
              "      <td>1.370361e-11</td>\n",
              "      <td>2.819056e-04</td>\n",
              "      <td>5.049787e-17</td>\n",
              "      <td>3.973487e-15</td>\n",
              "      <td>1.533638e-17</td>\n",
              "      <td>9.372481e-16</td>\n",
              "      <td>9.224742e-12</td>\n",
              "      <td>6.816786e-16</td>\n",
              "      <td>4.176001e-18</td>\n",
              "      <td>6.690498e-07</td>\n",
              "      <td>3.063687e-09</td>\n",
              "      <td>1.612659e-12</td>\n",
              "      <td>1.860438e-08</td>\n",
              "      <td>4.845653e-19</td>\n",
              "      <td>7.463254e-05</td>\n",
              "      <td>1.300109e-17</td>\n",
              "      <td>1.208043e-16</td>\n",
              "      <td>6.078177e-16</td>\n",
              "      <td>7.867657e-16</td>\n",
              "      <td>1.966624e-12</td>\n",
              "      <td>2.547993e-15</td>\n",
              "      <td>4.436560e-18</td>\n",
              "      <td>...</td>\n",
              "      <td>6.744940e-16</td>\n",
              "      <td>3.726127e-12</td>\n",
              "      <td>8.514658e-12</td>\n",
              "      <td>5.803135e-16</td>\n",
              "      <td>1.584360e-17</td>\n",
              "      <td>9.999576e-01</td>\n",
              "      <td>3.010772e-15</td>\n",
              "      <td>4.623767e-16</td>\n",
              "      <td>8.942671e-16</td>\n",
              "      <td>5.457323e-16</td>\n",
              "      <td>5.407974e-16</td>\n",
              "      <td>3.822270e-05</td>\n",
              "      <td>1.995251e-17</td>\n",
              "      <td>2.294191e-15</td>\n",
              "      <td>3.105012e-18</td>\n",
              "      <td>2.184581e-09</td>\n",
              "      <td>1.432983e-04</td>\n",
              "      <td>5.730152e-16</td>\n",
              "      <td>1.115727e-15</td>\n",
              "      <td>1.421338e-17</td>\n",
              "      <td>2.184581e-09</td>\n",
              "      <td>4.187167e-18</td>\n",
              "      <td>7.146017e-16</td>\n",
              "      <td>2.885606e-18</td>\n",
              "      <td>6.116522e-16</td>\n",
              "      <td>9.999712e-01</td>\n",
              "      <td>2.232252e-09</td>\n",
              "      <td>1.062877e-15</td>\n",
              "      <td>6.537029e-16</td>\n",
              "      <td>1.723434e-15</td>\n",
              "      <td>9.999408e-01</td>\n",
              "      <td>7.466144e-16</td>\n",
              "      <td>2.085189e-17</td>\n",
              "      <td>1.100699e-06</td>\n",
              "      <td>9.470663e-18</td>\n",
              "      <td>5.721536e-16</td>\n",
              "      <td>8.029237e-05</td>\n",
              "      <td>3.211803e-18</td>\n",
              "      <td>2.163027e-09</td>\n",
              "      <td>3.808054e-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3.308867e-10</td>\n",
              "      <td>5.086139e-04</td>\n",
              "      <td>1.174115e-08</td>\n",
              "      <td>2.457073e-03</td>\n",
              "      <td>1.936470e-10</td>\n",
              "      <td>3.616164e-10</td>\n",
              "      <td>1.569138e-08</td>\n",
              "      <td>2.598313e-10</td>\n",
              "      <td>2.241609e-12</td>\n",
              "      <td>4.966889e-05</td>\n",
              "      <td>9.991115e-01</td>\n",
              "      <td>2.835908e-10</td>\n",
              "      <td>3.061722e-10</td>\n",
              "      <td>3.060580e-10</td>\n",
              "      <td>1.095399e-10</td>\n",
              "      <td>4.600459e-05</td>\n",
              "      <td>7.022286e-05</td>\n",
              "      <td>3.206822e-10</td>\n",
              "      <td>9.991486e-01</td>\n",
              "      <td>1.315447e-12</td>\n",
              "      <td>1.211570e-04</td>\n",
              "      <td>2.480819e-10</td>\n",
              "      <td>8.499630e-05</td>\n",
              "      <td>3.091109e-10</td>\n",
              "      <td>9.991230e-01</td>\n",
              "      <td>4.433398e-05</td>\n",
              "      <td>1.030938e-10</td>\n",
              "      <td>1.306043e-03</td>\n",
              "      <td>1.755072e-04</td>\n",
              "      <td>1.467542e-08</td>\n",
              "      <td>4.520460e-09</td>\n",
              "      <td>9.769052e-08</td>\n",
              "      <td>3.116730e-07</td>\n",
              "      <td>7.462333e-05</td>\n",
              "      <td>1.034198e-04</td>\n",
              "      <td>3.143736e-10</td>\n",
              "      <td>4.494130e-05</td>\n",
              "      <td>7.824931e-09</td>\n",
              "      <td>4.007908e-05</td>\n",
              "      <td>1.025921e-10</td>\n",
              "      <td>...</td>\n",
              "      <td>4.454367e-05</td>\n",
              "      <td>3.521264e-09</td>\n",
              "      <td>9.990669e-01</td>\n",
              "      <td>3.243783e-10</td>\n",
              "      <td>8.730323e-05</td>\n",
              "      <td>3.626068e-18</td>\n",
              "      <td>2.859780e-10</td>\n",
              "      <td>4.328381e-10</td>\n",
              "      <td>3.050572e-10</td>\n",
              "      <td>3.854878e-10</td>\n",
              "      <td>4.271570e-05</td>\n",
              "      <td>4.329172e-05</td>\n",
              "      <td>8.932333e-05</td>\n",
              "      <td>2.841473e-10</td>\n",
              "      <td>1.487221e-10</td>\n",
              "      <td>4.540416e-04</td>\n",
              "      <td>2.210059e-12</td>\n",
              "      <td>3.094415e-10</td>\n",
              "      <td>2.435230e-10</td>\n",
              "      <td>9.625399e-05</td>\n",
              "      <td>4.540416e-04</td>\n",
              "      <td>1.182212e-10</td>\n",
              "      <td>2.887420e-10</td>\n",
              "      <td>1.406901e-10</td>\n",
              "      <td>2.842297e-10</td>\n",
              "      <td>1.796113e-18</td>\n",
              "      <td>5.517203e-04</td>\n",
              "      <td>5.070756e-05</td>\n",
              "      <td>4.344934e-05</td>\n",
              "      <td>5.496855e-05</td>\n",
              "      <td>4.070116e-18</td>\n",
              "      <td>4.481867e-05</td>\n",
              "      <td>9.344765e-05</td>\n",
              "      <td>1.419579e-04</td>\n",
              "      <td>1.607892e-06</td>\n",
              "      <td>3.414659e-10</td>\n",
              "      <td>1.821004e-12</td>\n",
              "      <td>1.307452e-10</td>\n",
              "      <td>4.298667e-04</td>\n",
              "      <td>3.512407e-09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 13472 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              7             3  ...             3             0\n",
              "0  0.000000e+00  1.561677e-06  ...  1.255984e-06  4.694674e-01\n",
              "1  0.000000e+00  3.185468e-03  ...  2.501099e-03  4.242348e-01\n",
              "2  0.000000e+00  7.913645e-05  ...  5.920060e-05  1.377271e-04\n",
              "3  2.814949e-34  9.777549e-01  ...  9.731141e-01  3.636517e-04\n",
              "4  5.272422e-01  8.974229e-09  ...  8.090341e-09  5.869216e-14\n",
              "5  1.872099e-21  1.847034e-02  ...  2.389454e-02  1.057965e-01\n",
              "6  2.223515e-01  1.210822e-13  ...  1.014432e-13  4.849143e-20\n",
              "7  2.504062e-01  4.130081e-09  ...  3.264986e-09  1.146094e-14\n",
              "8  1.447484e-15  2.004148e-09  ...  2.163027e-09  3.808054e-12\n",
              "9  3.308867e-10  5.086139e-04  ...  4.298667e-04  3.512407e-09\n",
              "\n",
              "[10 rows x 13472 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d5UTMx1t0V7",
        "outputId": "4f3ca37f-9498-4f26-b6c6-29f1ec3d263e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# keep only 10 rows from dataframe which contain the label names with highest probability values\n",
        "df_10_classes = pd.DataFrame()\n",
        "for col in range(df.shape[1]):\n",
        "  df_col = df.iloc[:,col]\n",
        "  df_max = df_col.nlargest(1)\n",
        "  df_max_index = df_max.index\n",
        "  df_10_classes[col] = df_max_inde\n",
        "  x\n",
        "\n",
        "# rename columns to labels of each video\n",
        "df_10_classes.columns = y_test_labels\n",
        "df_10_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>7</th>\n",
              "      <th>3</th>\n",
              "      <th>1</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>6</th>\n",
              "      <th>0</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>1</th>\n",
              "      <th>9</th>\n",
              "      <th>6</th>\n",
              "      <th>4</th>\n",
              "      <th>7</th>\n",
              "      <th>5</th>\n",
              "      <th>2</th>\n",
              "      <th>7</th>\n",
              "      <th>6</th>\n",
              "      <th>9</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>9</th>\n",
              "      <th>2</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>3</th>\n",
              "      <th>1</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>2</th>\n",
              "      <th>1</th>\n",
              "      <th>1</th>\n",
              "      <th>4</th>\n",
              "      <th>...</th>\n",
              "      <th>1</th>\n",
              "      <th>0</th>\n",
              "      <th>9</th>\n",
              "      <th>4</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>7</th>\n",
              "      <th>4</th>\n",
              "      <th>6</th>\n",
              "      <th>6</th>\n",
              "      <th>2</th>\n",
              "      <th>5</th>\n",
              "      <th>7</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>7</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>3</th>\n",
              "      <th>2</th>\n",
              "      <th>2</th>\n",
              "      <th>2</th>\n",
              "      <th>8</th>\n",
              "      <th>1</th>\n",
              "      <th>7</th>\n",
              "      <th>5</th>\n",
              "      <th>4</th>\n",
              "      <th>6</th>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <th>3</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 13472 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   7  3  1  4  4  6  0  7  7  1  9  6  4  ...  2  2  2  8  1  7  5  4  6  4  4  3  0\n",
              "0  4  3  0  4  4  4  0  4  4  1  9  4  4  ...  1  1  1  8  1  7  5  7  4  4  4  3  0\n",
              "\n",
              "[1 rows x 13472 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o8lg059yKWT",
        "outputId": "06e3784f-06ee-4016-d658-f0746e1fb919",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 3, 1, ..., 4, 3, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcWyibvP0N7s"
      },
      "source": [
        "#decoding the predicted value\n",
        "label_dict = { 0 : 'confident',\n",
        "               1 : 'unconfident',\n",
        "               2 : 'pos_hp', \n",
        "               3 : 'neg_hp', \n",
        "               4 : 'interested',\n",
        "               5 : 'uninterested', \n",
        "               6 : 'happy', \n",
        "               7 : 'unhappy',\n",
        "               8 : 'friendly',\n",
        "               9 : 'unfriendly'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ILb2ZzQyLTF"
      },
      "source": [
        "df_pred_new = pd.DataFrame(data=y_test_labels, columns=['Predicted Value'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47SBYyXQyVhb"
      },
      "source": [
        "df_pred_new['Predicted Category'] = df_pred_new['Predicted Value'].map(label_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhW6nIv_08cg",
        "outputId": "82aff949-397b-4ace-f808-ed1b8aae5dac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "df_pred_new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted Value</th>\n",
              "      <th>Predicted Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>unhappy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>neg_hp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>unconfident</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>interested</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>interested</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13467</th>\n",
              "      <td>6</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13468</th>\n",
              "      <td>4</td>\n",
              "      <td>interested</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13469</th>\n",
              "      <td>4</td>\n",
              "      <td>interested</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13470</th>\n",
              "      <td>3</td>\n",
              "      <td>neg_hp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13471</th>\n",
              "      <td>0</td>\n",
              "      <td>confident</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13472 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Predicted Value Predicted Category\n",
              "0                    7            unhappy\n",
              "1                    3             neg_hp\n",
              "2                    1        unconfident\n",
              "3                    4         interested\n",
              "4                    4         interested\n",
              "...                ...                ...\n",
              "13467                6              happy\n",
              "13468                4         interested\n",
              "13469                4         interested\n",
              "13470                3             neg_hp\n",
              "13471                0          confident\n",
              "\n",
              "[13472 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}