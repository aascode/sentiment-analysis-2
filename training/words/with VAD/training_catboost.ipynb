{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_catboost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hCJ7VTq1r-B"
      },
      "source": [
        "---\n",
        "# **Sentiment Analysis**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7xFZz0Z99Xz"
      },
      "source": [
        "---\n",
        "# **1. Installation and import**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6vZtO0WFbWr",
        "outputId": "1fa48cbb-f8db-43b9-dfd9-6cd6b8b2f0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "pip install catboost"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/39/128fff65072c8327371e3c594f3c826d29c85b21cb6485980353b168e0e4/catboost-0.24.2-cp36-none-manylinux1_x86_64.whl (66.1MB)\n",
            "\u001b[K     |████████████████████████████████| 66.2MB 46kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.18.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.24.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g46Leo44pFKm",
        "outputId": "e5890338-3607-4c4e-b556-89a3f3e85407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "! pip install glove_python"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting glove_python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 6.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 102kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 153kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 163kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 204kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 215kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 256kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.4.1)\n",
            "Building wheels for collected packages: glove-python\n",
            "  Building wheel for glove-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glove-python: filename=glove_python-0.1.0-cp36-cp36m-linux_x86_64.whl size=702584 sha256=a258a2d27a1cda2057cc809aad6b0e8c10f442638ebae24eb7fe5ca4902849b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
            "Successfully built glove-python\n",
            "Installing collected packages: glove-python\n",
            "Successfully installed glove-python-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFNjTuvU1_C_"
      },
      "source": [
        "## ii. Importing modules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIh873U819qU"
      },
      "source": [
        "# Data analysis\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import re\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator # to create a Word Cloud\n",
        "from PIL import Image # Pillow with WordCloud to image manipulation\n",
        "\n",
        "#embeddings \n",
        "import glove\n",
        "\n",
        "#models and metrics\n",
        "from sklearn.model_selection import *\n",
        "from sklearn.metrics import *\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65vv-0JxQsUK"
      },
      "source": [
        "---\n",
        "# **2. Parameters**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgAto0eNQtHH"
      },
      "source": [
        "# dimension of our embedding\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# features needed from the dataset\n",
        "features = [\"Valence\",\"Arousal\",\"Dominance\"] # \"quadrant\"\n",
        "\n",
        "# classes\n",
        "main_labels = ['confident', 'unconfident', \n",
        "               'pos_hp', 'neg_hp', \n",
        "               'interested', 'uninterested', \n",
        "               'happy', 'unhappy', \n",
        "               'friendly', 'unfriendly']\n",
        "               \n",
        "label_dict = dict(zip(main_labels, range(1, len(main_labels) + 1)))\n",
        "\n",
        "# inverting label_dict\n",
        "inv_label = {v: k for k, v in label_dict.items()}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yfg8TVR2SL_"
      },
      "source": [
        "---\n",
        "# **3. Load dataset**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUCh3RCa2VS8"
      },
      "source": [
        "def load_clean_dataset():\n",
        "    \"\"\"\n",
        "    function that loads the dataframe\n",
        "\n",
        "    return : df -> dataframe \n",
        "    \"\"\"\n",
        "    !mkdir -p datasets\n",
        "    !wget -nc https://raw.githubusercontent.com/Y4rd13/sentiment-analysis/master/datasets/results/categories_dataset_min_counter.csv -P datasets\n",
        "    df = pd.read_csv('./datasets/categories_dataset_min_counter.csv', encoding='utf-8', index_col=0))\n",
        "\n",
        "    return df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBXO0sRvmgix",
        "outputId": "6910faef-5266-4717-c912-c14f95643551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = load_clean_dataset()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-25 09:17:09--  https://raw.githubusercontent.com/Y4rd13/sentiment-analysis/master/datasets/results/categories_dataset_min_counter.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2432190 (2.3M) [text/plain]\n",
            "Saving to: ‘datasets/categories_dataset_min_counter.csv’\n",
            "\n",
            "categories_dataset_ 100%[===================>]   2.32M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-10-25 09:17:09 (46.6 MB/s) - ‘datasets/categories_dataset_min_counter.csv’ saved [2432190/2432190]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8J8_91sRKwo"
      },
      "source": [
        "Loading the Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l--8x2m-mjue",
        "outputId": "d618b05f-433d-4b88-c49a-bea3fcc3fd9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os, requests, shutil\n",
        "\n",
        "glove_dir = './data/RNN/'\n",
        "glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n",
        "glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n",
        "\n",
        "# These are temporary files if we need to download it from the original source (slow)\n",
        "data_cache = './data/cache'\n",
        "glove_full_tar = 'glove.6B.zip'\n",
        "glove_full_50d = 'glove.6B.50d.txt'\n",
        "\n",
        "#force_download_from_original=False\n",
        "download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+glove_100k_50d\n",
        "original_url = 'http://nlp.stanford.edu/data/'+glove_full_tar\n",
        "\n",
        "if not os.path.isfile( glove_100k_50d_path ):\n",
        "    if not os.path.exists(glove_dir):\n",
        "        os.makedirs(glove_dir)\n",
        "    \n",
        "    # First, try to download a pre-prepared file directly...\n",
        "    response = requests.get(download_url, stream=True)\n",
        "    if response.status_code == requests.codes.ok:\n",
        "        print(\"Downloading 42Mb pre-prepared GloVE file from RedCatLabs\")\n",
        "        with open(glove_100k_50d_path, 'wb') as out_file:\n",
        "            shutil.copyfileobj(response.raw, out_file)\n",
        "    else:\n",
        "        # But, for some reason, RedCatLabs didn't give us the file directly\n",
        "        if not os.path.exists(data_cache):\n",
        "            os.makedirs(data_cache)\n",
        "        \n",
        "        if not os.path.isfile( os.path.join(data_cache, glove_full_50d) ):\n",
        "            zipfilepath = os.path.join(data_cache, glove_full_tar)\n",
        "            if not os.path.isfile( zipfilepath ):\n",
        "                print(\"Downloading 860Mb GloVE file from Stanford\")\n",
        "                response = requests.get(download_url, stream=True)\n",
        "                with open(zipfilepath, 'wb') as out_file:\n",
        "                    shutil.copyfileobj(response.raw, out_file)\n",
        "            if os.path.isfile(zipfilepath):\n",
        "                print(\"Unpacking 50d GloVE file from zip\")\n",
        "                import zipfile\n",
        "                zipfile.ZipFile(zipfilepath, 'r').extract(glove_full_50d, data_cache)\n",
        "\n",
        "        with open(os.path.join(data_cache, glove_full_50d), 'rt') as in_file:\n",
        "            with open(glove_100k_50d_path, 'wt') as out_file:\n",
        "                print(\"Reducing 50d GloVE file to first 100k words\")\n",
        "                for i, l in enumerate(in_file.readlines()):\n",
        "                    if i>=100000: break\n",
        "                    out_file.write(l)\n",
        "    \n",
        "        # Get rid of tarfile source (the required text file itself will remain)\n",
        "        #os.unlink(zipfilepath)\n",
        "        #os.unlink(os.path.join(data_cache, glove_full_50d))\n",
        "\n",
        "print(\"GloVE available locally\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 42Mb pre-prepared GloVE file from RedCatLabs\n",
            "GloVE available locally\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKwX4H0do-KI",
        "outputId": "1ea8849a-c47b-43aa-c332-d051f5d39397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Due to size constraints, only use the first 100k vectors (i.e. 100k most frequently used words)\n",
        "word_embedding = glove.Glove.load_stanford( glove_100k_50d_path )\n",
        "word_embedding.word_vectors.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxYDBd5JpU6e"
      },
      "source": [
        "def get_embedding_vec(word):\n",
        "    \"\"\"\n",
        "    return : embedding vector of a word\n",
        "    \"\"\"\n",
        "    idx = word_embedding.dictionary.get(word.lower(), -1)\n",
        "    if idx<0:\n",
        "        #print(\"Missing word : '%s'\" % (word,))\n",
        "        return np.zeros(  (EMBEDDING_DIM, ), dtype='float32')  # UNK\n",
        "    return word_embedding.word_vectors[idx]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQxuHqWSAIsF"
      },
      "source": [
        "def prepare_x_y(df,features):\n",
        "  \"\"\"\n",
        "  function that creates X (features) and Y (labels)\n",
        "\n",
        "  return : x,y\n",
        "  \"\"\"\n",
        "  embedding_features =[]\n",
        "  for word in df[\"word\"]:\n",
        "    embedding_features.append(get_embedding_vec(word))\n",
        "\n",
        "  feats_without_embedding = df[features]\n",
        "  y = np.array(list(map(lambda x: label_dict[x.replace(' ', '_')], df[\"category\"])))\n",
        "\n",
        "  x = np.concatenate((feats_without_embedding,embedding_features),axis=1)\n",
        "\n",
        "  return x, y"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH4wDqKfR57K"
      },
      "source": [
        "---\n",
        "# **4. Training**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vPs4YlGQWTP"
      },
      "source": [
        "Preparation of X (features) and Y (labels) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUBiAJpc8vm4"
      },
      "source": [
        "x, y = prepare_x_y(df, features)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCSxivf_QTGw"
      },
      "source": [
        "We need to install catboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSRvdp-JCPqf",
        "outputId": "f9492bc8-1601-409d-88b4-eadaec191994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "oof_pred = np.zeros((len(x), 1))\n",
        "y_pred = np.zeros((len(y), 1))\n",
        "acc_score = []\n",
        "n_splits = 5\n",
        "i = 1\n",
        "skf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Using Kfold as our crossvalidation, and catboost as our training model\n",
        "for fold, (tr_ind, val_ind) in enumerate(skf.split(x, y)):\n",
        "\n",
        "    X_train, X_val = x[tr_ind], x[val_ind]\n",
        "    y_train, y_val = y[tr_ind], y[val_ind]\n",
        "\n",
        "    model = CatBoostClassifier(n_estimators=100, random_state=42, silent=True, task_type=\"GPU\", )\n",
        "    model.fit(X_train,y_train,eval_set=(X_val,y_val))\n",
        "\n",
        "    val_pred = model.predict(X_val)\n",
        "    print('validation Accuracy fold-', fold + 1, ': ', accuracy_score(y_val, val_pred))\n",
        "\n",
        "    acc_score.append(accuracy_score(y_val, val_pred))\n",
        "    oof_pred[val_ind] = val_pred\n",
        "\n",
        "    model.save_model(f\"fold{i}_catboost\")\n",
        "    #here predict on your test dataset\n",
        "    #y_pred += model.predict(test_df) / (n_splits)\n",
        "    i+=1\n",
        "\n",
        "print(\"Mean Accuracy = \", np.mean(acc_score,0))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation Accuracy fold- 1 :  0.8698714023270055\n",
            "validation Accuracy fold- 2 :  0.8663502755664422\n",
            "validation Accuracy fold- 3 :  0.8704837721984079\n",
            "validation Accuracy fold- 4 :  0.8686466625842009\n",
            "validation Accuracy fold- 5 :  0.8787507654623392\n",
            "Mean Accuracy =  0.8708205756276792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzSULiIGE0jJ"
      },
      "source": [
        "---\n",
        "# **5. Predicting**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svro1h67GDpE"
      },
      "source": [
        "def predict_class(word,valence,arousal,dominance):\n",
        "  \"\"\"\n",
        "  a function that predicts sentiments from words\n",
        "\n",
        "  ARGUMENTS : \n",
        "\n",
        "  word : our word (exple happy, tolerant ..)\n",
        "\n",
        "  valence,arousal,dominance,quadrant : important features that boost the accuracy of the model\n",
        "\n",
        "  return : class predicted\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  #creating the embedding of the word \n",
        "  embedding_features = get_embedding_vec(word)\n",
        "\n",
        "  feats_without_embedding = [valence, arousal, dominance]\n",
        "\n",
        "  #creating our X that contains all features (word_embeddings, VADQ)\n",
        "  x = np.concatenate((feats_without_embedding, embedding_features))\n",
        "  \n",
        "  #predictions (we will take saved models of each fold and do an average of predictions)\n",
        "  preds =[]\n",
        "  for i in range(1,n_splits+1):\n",
        "    model = CatBoostClassifier() \n",
        "    model.load_model(f'fold{i}_catboost')\n",
        "    predictions = model.predict_proba(x)\n",
        "    preds.append(predictions)\n",
        "  \n",
        "  final_preds = np.mean(preds,0)\n",
        "\n",
        "  return inv_label[np.argmax(final_preds) + 1]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gTIN1wEqVAh"
      },
      "source": [
        "def f_quad(V, A, D):\n",
        "    # I high arousal, positive valance\n",
        "    if A > 0.5 and V > 0.5:\n",
        "        quad = 1 \n",
        "    # II high arousal, negative valance\n",
        "    elif A > 0.5 and V < 0.5:\n",
        "        quad = 2\n",
        "    # III negative arousal, negative valance\n",
        "    elif A < 0.5 and V < 0.5:\n",
        "        quad = 3\n",
        "    # IV negative arousal, positive valance\n",
        "    elif A < 0.5 and V > 0.5:\n",
        "        quad = 4\n",
        "    else:\n",
        "        quad = 0\n",
        "    return quad"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3UJkEvXqIMI"
      },
      "source": [
        "word = \"smile\"\n",
        "V, A, D = 0.906, 0.598, 0.618\n",
        "#quad = f_quad(V, A, D)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRsQiBGmI3UB",
        "outputId": "685d0f34-4735-471f-9b9b-a4f2c2def0b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "predict_class(word, V, A, D)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'confident'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}