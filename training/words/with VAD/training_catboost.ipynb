{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_catboost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hCJ7VTq1r-B"
      },
      "source": [
        "---\n",
        "# **Sentiment Analysis**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6vZtO0WFbWr",
        "outputId": "8d0a8994-f4b6-4e0b-b7db-822bb3808483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "pip install catboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/39/128fff65072c8327371e3c594f3c826d29c85b21cb6485980353b168e0e4/catboost-0.24.2-cp36-none-manylinux1_x86_64.whl (66.1MB)\n",
            "\u001b[K     |████████████████████████████████| 66.2MB 45kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.24.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g46Leo44pFKm",
        "outputId": "c2ed9242-1d42-47fc-a15c-00acbccf0d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "! pip install glove_python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting glove_python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 19.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 6.7MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 102kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 153kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 163kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 204kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 215kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 256kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.4.1)\n",
            "Building wheels for collected packages: glove-python\n",
            "  Building wheel for glove-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glove-python: filename=glove_python-0.1.0-cp36-cp36m-linux_x86_64.whl size=702609 sha256=f65873f443e8e78b5a5dbad5fe3cbe6c74c7cf85666fbcecd8893335e0f93d45\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
            "Successfully built glove-python\n",
            "Installing collected packages: glove-python\n",
            "Successfully installed glove-python-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFNjTuvU1_C_"
      },
      "source": [
        "## ii. Importing modules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIh873U819qU"
      },
      "source": [
        "# Data analysis\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import re\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator # to create a Word Cloud\n",
        "from PIL import Image # Pillow with WordCloud to image manipulation\n",
        "\n",
        "#embeddings \n",
        "import glove\n",
        "\n",
        "#models and metrics\n",
        "from sklearn.model_selection import *\n",
        "from sklearn.metrics import *\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65vv-0JxQsUK"
      },
      "source": [
        "---\n",
        "# **4. Parameters**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgAto0eNQtHH"
      },
      "source": [
        "# dimension of our embedding\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "# features needed from the dataset\n",
        "features = [\"Valence\",\"Arousal\",\"Dominance\",\"quadrant\"]\n",
        "\n",
        "# classes\n",
        "main_labels = ['confident', 'unconfident', \n",
        "               'can handle pressure', 'cannot handle pressure', \n",
        "               'interested', 'uninterested', \n",
        "               'happy', 'unhappy', \n",
        "               'friendly', 'unfriendly']\n",
        "               \n",
        "label_dict = dict(zip(main_labels, range(1, len(main_labels) + 1)))\n",
        "\n",
        "# inverting label_dict\n",
        "inv_label = {v: k for k, v in label_dict.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yfg8TVR2SL_"
      },
      "source": [
        "---\n",
        "# **4. Load dataset**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUCh3RCa2VS8"
      },
      "source": [
        "def load_clean_dataset():\n",
        "    \"\"\"\n",
        "    function that loads the dataframe\n",
        "\n",
        "    return : df -> dataframe \n",
        "    \"\"\"\n",
        "    !mkdir -p datasets\n",
        "    !wget -nc https://raw.githubusercontent.com/Y4rd13/sentiment-analysis/master/datasets/results/categories_dataset.csv -P datasets\n",
        "    df = pd.read_csv('./datasets/categories_dataset.csv', encoding='utf-8', index_col=0, dtype=({'score':float}))\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBXO0sRvmgix",
        "outputId": "58db7639-a51f-4fff-a665-2af6fba01a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df = load_clean_dataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-13 00:31:57--  https://raw.githubusercontent.com/Y4rd13/sentiment-analysis/master/datasets/results/categories_dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7464477 (7.1M) [text/plain]\n",
            "Saving to: ‘datasets/categories_dataset.csv’\n",
            "\n",
            "categories_dataset. 100%[===================>]   7.12M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-10-13 00:31:58 (48.6 MB/s) - ‘datasets/categories_dataset.csv’ saved [7464477/7464477]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8J8_91sRKwo"
      },
      "source": [
        "Loading the Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l--8x2m-mjue",
        "outputId": "7b6442ab-6710-4f7e-a36a-8d95d502e28b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import os, requests, shutil\n",
        "\n",
        "glove_dir = './data/RNN/'\n",
        "glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n",
        "glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n",
        "\n",
        "# These are temporary files if we need to download it from the original source (slow)\n",
        "data_cache = './data/cache'\n",
        "glove_full_tar = 'glove.6B.zip'\n",
        "glove_full_50d = 'glove.6B.50d.txt'\n",
        "\n",
        "#force_download_from_original=False\n",
        "download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+glove_100k_50d\n",
        "original_url = 'http://nlp.stanford.edu/data/'+glove_full_tar\n",
        "\n",
        "if not os.path.isfile( glove_100k_50d_path ):\n",
        "    if not os.path.exists(glove_dir):\n",
        "        os.makedirs(glove_dir)\n",
        "    \n",
        "    # First, try to download a pre-prepared file directly...\n",
        "    response = requests.get(download_url, stream=True)\n",
        "    if response.status_code == requests.codes.ok:\n",
        "        print(\"Downloading 42Mb pre-prepared GloVE file from RedCatLabs\")\n",
        "        with open(glove_100k_50d_path, 'wb') as out_file:\n",
        "            shutil.copyfileobj(response.raw, out_file)\n",
        "    else:\n",
        "        # But, for some reason, RedCatLabs didn't give us the file directly\n",
        "        if not os.path.exists(data_cache):\n",
        "            os.makedirs(data_cache)\n",
        "        \n",
        "        if not os.path.isfile( os.path.join(data_cache, glove_full_50d) ):\n",
        "            zipfilepath = os.path.join(data_cache, glove_full_tar)\n",
        "            if not os.path.isfile( zipfilepath ):\n",
        "                print(\"Downloading 860Mb GloVE file from Stanford\")\n",
        "                response = requests.get(download_url, stream=True)\n",
        "                with open(zipfilepath, 'wb') as out_file:\n",
        "                    shutil.copyfileobj(response.raw, out_file)\n",
        "            if os.path.isfile(zipfilepath):\n",
        "                print(\"Unpacking 50d GloVE file from zip\")\n",
        "                import zipfile\n",
        "                zipfile.ZipFile(zipfilepath, 'r').extract(glove_full_50d, data_cache)\n",
        "\n",
        "        with open(os.path.join(data_cache, glove_full_50d), 'rt') as in_file:\n",
        "            with open(glove_100k_50d_path, 'wt') as out_file:\n",
        "                print(\"Reducing 50d GloVE file to first 100k words\")\n",
        "                for i, l in enumerate(in_file.readlines()):\n",
        "                    if i>=100000: break\n",
        "                    out_file.write(l)\n",
        "    \n",
        "        # Get rid of tarfile source (the required text file itself will remain)\n",
        "        #os.unlink(zipfilepath)\n",
        "        #os.unlink(os.path.join(data_cache, glove_full_50d))\n",
        "\n",
        "print(\"GloVE available locally\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 42Mb pre-prepared GloVE file from RedCatLabs\n",
            "GloVE available locally\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKwX4H0do-KI",
        "outputId": "685cf1c1-0062-4119-8ac4-6da3b8e738c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Due to size constraints, only use the first 100k vectors (i.e. 100k most frequently used words)\n",
        "word_embedding = glove.Glove.load_stanford( glove_100k_50d_path )\n",
        "word_embedding.word_vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxYDBd5JpU6e"
      },
      "source": [
        "def get_embedding_vec(word):\n",
        "    \"\"\"\n",
        "    return : embedding vector of a word\n",
        "    \"\"\"\n",
        "    idx = word_embedding.dictionary.get(word.lower(), -1)\n",
        "    if idx<0:\n",
        "        #print(\"Missing word : '%s'\" % (word,))\n",
        "        return np.zeros(  (EMBEDDING_DIM, ), dtype='float32')  # UNK\n",
        "    return word_embedding.word_vectors[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQxuHqWSAIsF"
      },
      "source": [
        "def prepare_x_y(df,features):\n",
        "  \"\"\"\n",
        "  function that creates X (features) and Y (labels)\n",
        "\n",
        "  return : x,y\n",
        "  \"\"\"\n",
        "  embedding_features =[]\n",
        "  for word in df[\"word\"]:\n",
        "    embedding_features.append(get_embedding_vec(word))\n",
        "\n",
        "  feats_without_embedding = df[features]\n",
        "  y = np.array(list(map(lambda x: label_dict[x.replace(' ', '_')], df[\"category\"])))\n",
        "\n",
        "  x = np.concatenate((feats_without_embedding,embedding_features),axis=1)\n",
        "\n",
        "  return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH4wDqKfR57K"
      },
      "source": [
        "---\n",
        "# **4. Training**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vPs4YlGQWTP"
      },
      "source": [
        "Preparation of X (features) and Y (labels) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUBiAJpc8vm4",
        "outputId": "8da5dad2-6ef8-4877-ca88-0523942e6eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "x, y = prepare_x_y(df, features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7d7e2c50cd64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_x_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-40718fb49ce9>\u001b[0m in \u001b[0;36mprepare_x_y\u001b[0;34m(df, features)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mfeats_without_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats_without_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-40718fb49ce9>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mfeats_without_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats_without_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'pos_hp'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCSxivf_QTGw"
      },
      "source": [
        "We need to install catboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSRvdp-JCPqf"
      },
      "source": [
        "oof_pred = np.zeros((len(x), 1))\n",
        "y_pred = np.zeros((len(y), 1))\n",
        "acc_score = []\n",
        "n_splits = 5\n",
        "i = 1\n",
        "skf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Using Kfold as our crossvalidation, and catboost as our training model\n",
        "for fold, (tr_ind, val_ind) in enumerate(skf.split(x, y)):\n",
        "\n",
        "    X_train, X_val = x[tr_ind], x[val_ind]\n",
        "    y_train, y_val = y[tr_ind], y[val_ind]\n",
        "\n",
        "    model = CatBoostClassifier(n_estimators=100, random_state=42, silent=True, task_type=\"GPU\", )\n",
        "    model.fit(X_train,y_train,eval_set=(X_val,y_val))\n",
        "\n",
        "    val_pred = model.predict(X_val)\n",
        "    print('validation Accuracy fold-', fold + 1, ': ', accuracy_score(y_val, val_pred))\n",
        "\n",
        "    acc_score.append(accuracy_score(y_val, val_pred))\n",
        "    oof_pred[val_ind] = val_pred\n",
        "\n",
        "    model.save_model(f\"fold{i}_catboost\")\n",
        "    #here predict on your test dataset\n",
        "    #y_pred += model.predict(test_df) / (n_splits)\n",
        "    i+=1\n",
        "\n",
        "print(\"Mean Accuracy = \", np.mean(acc_score,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzSULiIGE0jJ"
      },
      "source": [
        "---\n",
        "# **4. Predicting**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svro1h67GDpE"
      },
      "source": [
        "def predict_class(word,valence,arousal,dominance,quadrant):\n",
        "  \"\"\"\n",
        "  a function that predicts sentiments from words\n",
        "\n",
        "  ARGUMENTS : \n",
        "\n",
        "  word : our word (exple happy, tolerant ..)\n",
        "\n",
        "  valence,arousal,dominance,quadrant : important features that boost the accuracy of the model\n",
        "\n",
        "  return : class predicted\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  #creating the embedding of the word \n",
        "  embedding_features = get_embedding_vec(word)\n",
        "\n",
        "  feats_without_embedding = [valence, arousal, dominance, quadrant]\n",
        "\n",
        "  #creating our X that contains all features (word_embeddings, VADQ)\n",
        "  x = np.concatenate((feats_without_embedding,embedding_features))\n",
        "  \n",
        "  #predictions (we will take saved models of each fold and do an average of predictions)\n",
        "  preds =[]\n",
        "  for i in range(1,n_splits+1):\n",
        "    model = CatBoostClassifier() \n",
        "    model.load_model(f'fold{i}_catboost')\n",
        "    predictions = model.predict_proba(x)\n",
        "    preds.append(predictions)\n",
        "  \n",
        "  final_preds = np.mean(preds,0)\n",
        "\n",
        "  return inv_label[np.argmax(final_preds) + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gTIN1wEqVAh"
      },
      "source": [
        "def f_quad(V, A, D):\n",
        "    # I high arousal, positive valance\n",
        "    if A > 0.5 and V > 0.5:\n",
        "        quad = 1 \n",
        "    # II high arousal, negative valance\n",
        "    elif A > 0.5 and V < 0.5:\n",
        "        quad = 2\n",
        "    # III negative arousal, negative valance\n",
        "    elif A < 0.5 and V < 0.5:\n",
        "        quad = 3\n",
        "    # IV negative arousal, positive valance\n",
        "    elif A < 0.5 and V > 0.5:\n",
        "        quad = 4\n",
        "    else:\n",
        "        quad = 0\n",
        "    return quad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3UJkEvXqIMI"
      },
      "source": [
        "word = \"smile\"\n",
        "V, A, D = 0.906, 0.598, 0.618\n",
        "quad = f_quad(V, A, D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRsQiBGmI3UB"
      },
      "source": [
        "predict_class(word, V, A, D, quad)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}