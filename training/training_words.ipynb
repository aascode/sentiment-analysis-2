{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_words.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hCJ7VTq1r-B"
      },
      "source": [
        "---\n",
        "# **Sentiment Analysis**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrfoCSVi1uU_"
      },
      "source": [
        "---\n",
        "\n",
        "# **1. Installation**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfMMl1bb1x_2"
      },
      "source": [
        "## i. Generating a reponse\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soQns5KH0mNV"
      },
      "source": [
        "import sys\n",
        "import logging\n",
        "from psutil import virtual_memory"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwXk46ge1zYO"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "ram_gb = virtual_memory().total / 1e9"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqYgi67z1zgZ"
      },
      "source": [
        "tf_response = {\n",
        "    'error': None,\n",
        "    'TF version': '',\n",
        "    'COLAB': None,\n",
        "    'GPU': False,\n",
        "    'ram_gb': ''\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvNt2AJ51znJ",
        "outputId": "30527ebf-7db7-41f0-c37a-97dd9ba38055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "try:\n",
        "    # drive\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "    # updating tensorflow version\n",
        "    %tensorflow_version 2.x\n",
        "\n",
        "    # tensorflow-gpu & tensorflowjs\n",
        "    !pip install tensorflow-gpu # !pip install tensorflow_text # I could use BERT\n",
        "    !pip install tensorflowjs\n",
        "\n",
        "    # NLP (nltk, stanza, spacy)\n",
        "    !pip install nltk \n",
        "    !pip install stanza\n",
        "    !pip install spacy\n",
        "    !spacy download en_core_web_sm # sm md lg\n",
        "    !python -m spacy download en\n",
        "except OSError as error:\n",
        "    # debugging error\n",
        "    response['error'] = logging.debug('You are not using your specify version of TensorFlow')\n",
        "    IN_COLAB = False\n",
        "\n",
        "    # install requirements\n",
        "    !pip install -r '../requirements.txt'\n",
        "finally:\n",
        "    tf_response['COLAB'] = IN_COLAB\n",
        "    \n",
        "    # Importing tensroflow core\n",
        "    import tensorflow as tf\n",
        "    import tensorflowjs as tfjs\n",
        "    from tensorflow import keras\n",
        "    from keras.utils import to_categorical\n",
        "\n",
        "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional, GlobalMaxPool1D\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    \n",
        "    # GPU and RAM response\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        GPU = True\n",
        "        tf_response['GPU'] = GPU\n",
        "        tf_response['TF_version'] = tf.__version__\n",
        "        \n",
        "        if tf_response['COLAB'] == True:\n",
        "            if gpu_info.find('failed') >= 0:\n",
        "                print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator')\n",
        "                print('Re-execute this cell.')\n",
        "            else:\n",
        "                print(gpu_info)\n",
        "            \n",
        "            if ram_gb < 20:\n",
        "                print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type menu\"')\n",
        "                print('Select high-RAM in the runtime shape dropdown')\n",
        "                print('Re-execute this cell')\n",
        "                tf_response['ram_gb'] = 'low-RAM runtime'\n",
        "            else:\n",
        "                tf_response['ram_gb'] = 'high-RAM runtime'\n",
        "            print('\\nRuntime {:.2f} GB of available RAM\\n'.format(ram_gb))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.35.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.10.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (50.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2020.6.20)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: h5py>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.10.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.18.5)\n",
            "Requirement already satisfied: PyInquirer==1.0.3 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.0.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-cpu<3,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.3.1)\n",
            "Requirement already satisfied: tensorflow-hub==0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (0.7.0)\n",
            "Requirement already satisfied: Pygments>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from PyInquirer==1.0.3->tensorflowjs) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit==1.0.14 in /usr/local/lib/python3.6/dist-packages (from PyInquirer==1.0.3->tensorflowjs) (1.0.14)\n",
            "Requirement already satisfied: regex>=2016.11.21 in /usr/local/lib/python3.6/dist-packages (from PyInquirer==1.0.3->tensorflowjs) (2019.12.20)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.12.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.35.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit==1.0.14->PyInquirer==1.0.3->tensorflowjs) (0.2.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (50.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (4.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.6.0+cu101)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (50.3.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye99iKv617qQ",
        "outputId": "5ee834f1-91d3-4aa3-a60f-4ea9dec6f58c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf_response"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'COLAB': True, 'GPU': False, 'TF version': '', 'error': None, 'ram_gb': ''}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFNjTuvU1_C_"
      },
      "source": [
        "## ii. Importing modules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIh873U819qU"
      },
      "source": [
        "# Data analysis\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import re\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator # to create a Word Cloud\n",
        "from PIL import Image # Pillow with WordCloud to image manipulation"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lhfhNt72CLr",
        "outputId": "38b8f5af-c3ac-4fcf-c44c-fdb070614461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T82WGVnN2EwF",
        "outputId": "f58bd546-7723-4518-9b04-2b6874cccc0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "# Stanza NLP\n",
        "import stanza\n",
        "\n",
        "stanza.download('en', package='ewt', processors='tokenize,mwt,pos,lemma', verbose=True)\n",
        "stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma',\n",
        "                      lang='en',\n",
        "                      use_gpu=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 9.10MB/s]                    \n",
            "2020-09-25 19:09:19 WARNING: Can not find mwt: ewt from official model list. Ignoring it.\n",
            "2020-09-25 19:09:19 INFO: Downloading these customized packages for language: en (English)...\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "| pretrain  | ewt     |\n",
            "=======================\n",
            "\n",
            "2020-09-25 19:09:19 INFO: File exists: /root/stanza_resources/en/tokenize/ewt.pt.\n",
            "2020-09-25 19:09:19 INFO: File exists: /root/stanza_resources/en/pos/ewt.pt.\n",
            "2020-09-25 19:09:19 INFO: File exists: /root/stanza_resources/en/lemma/ewt.pt.\n",
            "2020-09-25 19:09:19 INFO: File exists: /root/stanza_resources/en/pretrain/ewt.pt.\n",
            "2020-09-25 19:09:19 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2020-09-25 19:09:19 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
            "2020-09-25 19:09:19 INFO: Loading these models for language: en (English):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "=======================\n",
            "\n",
            "2020-09-25 19:09:19 INFO: Use device: gpu\n",
            "2020-09-25 19:09:19 INFO: Loading: tokenize\n",
            "2020-09-25 19:09:24 INFO: Loading: pos\n",
            "2020-09-25 19:09:25 INFO: Loading: lemma\n",
            "2020-09-25 19:09:25 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9236VgMj2Gwg",
        "outputId": "cc55f049-9c73-4cd6-8d49-20e1a4de594c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# testing stanza\n",
        "doc = stNLP('Barack Obama was born in Hawai.')\n",
        "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word: Barack \tlemma: Barack\n",
            "word: Obama \tlemma: Obama\n",
            "word: was \tlemma: be\n",
            "word: born \tlemma: bear\n",
            "word: in \tlemma: in\n",
            "word: Hawai \tlemma: Hawai\n",
            "word: . \tlemma: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4QpRgR02IDL"
      },
      "source": [
        "# Spacy NLP\n",
        "import spacy\n",
        "spNLP = spacy.load('en_core_web_sm')\n",
        "spNLP.max_length = 103950039 # or higher\n",
        "# spacy.prefer_gpu() #will not work with stanza"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6iiQVT42Jo3"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# **2. Hyperparameters**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIpxO61l0gcD"
      },
      "source": [
        "FILE = 'datasets/categories_dataset.csv'\n",
        "MODEL_PATH = './models/model.h5'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUv__06r2LOQ"
      },
      "source": [
        "EPOCHS = 300\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "training_portion = .8\n",
        "rnn_units = 64\n",
        "\n",
        "# embedding_dim : len(words) + 1\n",
        "# vocab_size : len(words.index) + 1\n",
        "vocab_size = 5000\n",
        "\n",
        "max_lenght = 30 # 200 # for padding_shape on preprocess function\n",
        "\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "padding_shape = 30\n",
        "oov_tok = '<OOV>'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBKrSlNb0fIs"
      },
      "source": [
        "main_labels = ['confident', 'unconfident', \n",
        "               'pos_hp', 'neg_hp', \n",
        "               'interested', 'uninterested', \n",
        "               'happy', 'unhappy', \n",
        "               'friendly', 'unfriendly', \n",
        "               'POS']\n",
        "               \n",
        "label_dict = dict(zip(main_labels, range(1, len(main_labels) + 1)))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyVrjQ46nEMd",
        "outputId": "369611cb-e378-42b4-9d0a-6d35e8e05219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "label_dict"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'POS': 11,\n",
              " 'confident': 1,\n",
              " 'friendly': 9,\n",
              " 'happy': 7,\n",
              " 'interested': 5,\n",
              " 'neg_hp': 4,\n",
              " 'pos_hp': 3,\n",
              " 'unconfident': 2,\n",
              " 'unfriendly': 10,\n",
              " 'unhappy': 8,\n",
              " 'uninterested': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO32c_pD2NzO"
      },
      "source": [
        "---\n",
        "\n",
        "# **3. Lemmatization**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb6Wbv0U2OeH"
      },
      "source": [
        "# lemmatizion\n",
        "# stanza\n",
        "def stanza_lemma(text):\n",
        "    doc = stNLP(text)\n",
        "    return ' '.join([word.lemma for sent in doc.sentences for word in sent.words])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdtBcgKB2PaB"
      },
      "source": [
        "def nltk_lemma(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatizer.lemmatize(text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yfg8TVR2SL_"
      },
      "source": [
        "---\n",
        "# **4. Load dataset**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUCh3RCa2VS8"
      },
      "source": [
        "def load_clean_dataset():\n",
        "    !mkdir -p datasets\n",
        "    !wget -nc https://raw.githubusercontent.com/Y4rd13/sentiment-analysis/master/datasets/results/categories_dataset.csv -P datasets\n",
        "    df = pd.read_csv('./datasets/categories_dataset.csv', encoding='utf-8', index_col=0, dtype=({'score':float}))\n",
        "\n",
        "    print(df.head())\n",
        "    x, y = df['word'], df[['category', 'POS stza']] # or POS nltk\n",
        "    #x, y = np.array(x, dtype='<U33'), np.array(y, dtype='<U33')\n",
        "\n",
        "    return x, y"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kleN4LtF2-bg"
      },
      "source": [
        "---\n",
        "# **5. Prepare dataset**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDdZHwMcolpC"
      },
      "source": [
        "def preprocess(x, padding_shape=30):\n",
        "    return np.array([ord(i.lower()) - ord('a')+1 if not i.isdigit() and i != ' ' else 0 for i in list(x)] + ([0] * (padding_shape - len(x))), dtype=int)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y-Y8HxI3E7B"
      },
      "source": [
        "def prepare_dataset(labeldict : dict, test_size=.3, validation_size=.1): \n",
        "    print('preparing the dataset...\\n')\n",
        "    \n",
        "    from sklearn import preprocessing\n",
        "\n",
        "    # load dataset\n",
        "    # split dataset (as string into panda.core.series.Serie object)\n",
        "    x, y = load_clean_dataset()\n",
        "\n",
        "    x = np.array(list(map(preprocess, x)))\n",
        "    y = np.array(list(map(lambda x: labeldict[x.replace(' ', '_')], y['category'])))\n",
        "    print(('y: {}').format(y))\n",
        "\n",
        "    # create/split train, validation and test and shuffle the data\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, shuffle=True, \n",
        "                                                        random_state=42) # use random_state=42 ? **\n",
        "    print(x.max(), x.min())\n",
        "\n",
        "    x_train_val, x_validation, y_train_val, y_validation = train_test_split(x_train, y_train, test_size=test_size, shuffle=True)\n",
        "\n",
        "    # pandas.core.series.Series to numpy array\n",
        "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "    x_validation, y_validation =  np.array(x_validation), np.array(y_validation)\n",
        "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
        "    \n",
        "    x_train_val, y_train_val = np.array(x_train_val), np.array(y_train_val)\n",
        "\n",
        "    print(('\\nx_train: \\n{}\\n\\ny_train: \\n{}').format(x_train_val, y_train_val))\n",
        "\n",
        "    return (x_train, y_train), (x_validation, y_validation), (x_test, y_test), (x_train_val, y_train_val)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpHOyVkIwY4r",
        "outputId": "ecb66a3f-2ba1-4873-9647-7c0dd936507c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "(x_train, y_train), (x_validation, y_validation), (x_test, y_test), (x_train_val, y_train_val) = prepare_dataset(label_dict)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preparing the dataset...\n",
            "\n",
            "File ‘datasets/categories_dataset.csv’ already there; not retrieving.\n",
            "\n",
            "    category          word  score  ...  POS stza  POS nlkt  POS spcy\n",
            "0  confident     tolerance   2.63  ...      NOUN      NOUN      NOUN\n",
            "1  confident         carry   0.46  ...      VERB      NOUN      VERB\n",
            "2  confident   tranquility   1.00  ...      NOUN      NOUN      NOUN\n",
            "3  confident  socontagious   2.69  ...       ADJ       ADJ       ADJ\n",
            "4  confident          zeal   1.00  ...      NOUN      NOUN     PROPN\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "y: [ 1  1  1 ... 10 10 10]\n",
            "26 0\n",
            "\n",
            "x_train: \n",
            "[[18  5 20 ...  0  0  0]\n",
            " [ 4  9 19 ...  0  0  0]\n",
            " [ 1 14 25 ...  0  0  0]\n",
            " ...\n",
            " [ 3 15 14 ...  0  0  0]\n",
            " [17 21  1 ...  0  0  0]\n",
            " [20  8  5 ...  0  0  0]]\n",
            "\n",
            "y_train: \n",
            "[ 3  4 10 ...  2 10  5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQEwk09b4PD_"
      },
      "source": [
        "---\n",
        "# **4. Build model**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD18U_yE4SOA"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim=64, input_length=30):\n",
        "    print('\\nbuilding the model...\\n')\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=(vocab_size + 1), output_dim=embedding_dim, input_length=input_length),\n",
        "        \n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units,return_sequences=True, dropout=0.2)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units,return_sequences=True, dropout=0.2)),\n",
        "        tf.keras.layers.GlobalMaxPool1D(),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        \n",
        "        # softmax output layer\n",
        "        tf.keras.layers.Dense(11, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # optimizer & loss\n",
        "    opt = 'adam' #tf.optimizers.Adam(learning_rate=1e-4)\n",
        "    loss = 'categorical_crossentropy'\n",
        "\n",
        "    # Metrics\n",
        "    # Instead of using AUC: Precision & Recall\n",
        "    # Precision: Computes the precision of the predictions with respect to the labels.\n",
        "    # Recall: Computes the recall of the predictions with respect to the labels.\n",
        "    metrics = ['accuracy', 'AUC','Precision', 'Recall', 'F1Score']\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=loss,\n",
        "                  metrics=metrics)\n",
        "    \n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf048l554Uug"
      },
      "source": [
        "---\n",
        "# **6. Train model** \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-pVOZYb4T3T"
      },
      "source": [
        "def train(model, x_train, y_train, x_validation, y_validation,\n",
        "          epochs, batch_size=32, patience=5, \n",
        "          verbose=2, monitor_es='accuracy', mode_es='auto', restore=True,\n",
        "          monitor_mc='val_acc', mode_mc='max'):\n",
        "    \n",
        "    print('\\ntraining...\\n')\n",
        "\n",
        "    # callback\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor=monitor_es,\n",
        "                                                      verbose=1, mode=mode_es, restore_best_weights=restore,\n",
        "                                                      min_delta=1e-3, patience=patience)\n",
        "    \n",
        "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint('tfjsmode.h5', monitor=monitor_mc, mode=mode_mc,      \n",
        "                                                          verbose=1, save_best_only=True)\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
        "                        validation_data=(x_validation, y_validation),\n",
        "                        callbacks=[early_stopping, model_checkpoint])\n",
        "    return history"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy3DyM-b4XPg"
      },
      "source": [
        "---\n",
        "# **7. Plotting history**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssFJ3CIe4YPn"
      },
      "source": [
        "def plot_history_(history):\n",
        "    fitModel_dict = history.history\n",
        "    acc = fitModel_dict['accuracy']\n",
        "    val_acc = fitModel_dict['val_accuracy']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
        "    plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.ylim((0.5, 1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_history(history, string):\n",
        "    fitModel_dict = history.history\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(fitModel_dict[string])\n",
        "    plt.plot(fitModel_dict['val_' + string])\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_' + string])\n",
        "    plt.show()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oScqNNBXsL7Y"
      },
      "source": [
        "---\n",
        "# **8. Prediction**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekhWVsv7sK7z"
      },
      "source": [
        "def predict(d : dict, s : str, model):\n",
        "    token = preprocess(s)\n",
        "    output = model.predict(np.array([token]))\n",
        "    id = int(tf.keras.backend.argmax(output))\n",
        "    print(output)\n",
        "\n",
        "    for k, v in d.items():\n",
        "        if v == id:\n",
        "            return k\n",
        "    return 'unclassified'"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FN5osTo4aC4"
      },
      "source": [
        "---\n",
        "# **9. Main**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAb369Lh4ajo"
      },
      "source": [
        "def main():\n",
        "    # prepare the dataset\n",
        "    (x_train, y_train), (x_validation, y_validation), (x_test, y_test) = prepare_dataset(label_dict)\n",
        "    print(x_train.shape, x_validation.shape, x_test.shape)\n",
        "    print(y_train.shape, y_validation.shape, x_test.shape)\n",
        "    print(y_train)\n",
        "\n",
        "    # build the model\n",
        "    model = build_model()\n",
        "\n",
        "    # train the model\n",
        "    history = train(model=model, x_train=x_train, y_train=to_categorical(y_train),\n",
        "                    x_validation=x_validation, y_validation=to_categorical(y_validation),\n",
        "                    epochs=EPOCHS, verbose=1, monitor='accuracy')\n",
        "\n",
        "    # plot the training\n",
        "    plot_history_(history)\n",
        "\n",
        "    plot_history(history, 'accuracy')\n",
        "\n",
        "    # evaluate the model\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "    evaluate_lst = list(map(lambda x: x * 100, [test_loss, test_accuracy]))\n",
        "    print('\\nTest:\\nLoss: {}\\nAccuracy: {}').format(evaluate_lst[0], evaluate_lst[1])\n",
        "\n",
        "    # predict\n",
        "    s = 'happy'\n",
        "    pred = predict(label_dict, s, model)\n",
        "    print('Word: {}'.format(s))\n",
        "    print('Prediction: {}'.format(pred))\n",
        "\n",
        "    # save the model\n",
        "    !mkdir models\n",
        "    model.save(MODEL_PATH)\n",
        "\n",
        "    # convert modeljs\n",
        "    tfjs.converters.save_keras_model(model, 'tfjsmodel') "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Lry1jA4b4w"
      },
      "source": [
        "---\n",
        "# **10. testing**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1BALgvxi3_U",
        "outputId": "bb9e7e15-4559-4c42-ded4-08d0a9a97d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "# prepare the dataset\n",
        "(x_train, y_train), (x_validation, y_validation), (x_test, y_test), (x_test_val, y_test_val) = prepare_dataset(label_dict)\n",
        "print(x_train.shape, x_validation.shape, x_test.shape)\n",
        "print(y_train.shape, y_validation.shape, x_test.shape)\n",
        "print(y_train)\n",
        "\n",
        "# build the model\n",
        "model = build_model(vocab_size)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preparing the dataset...\n",
            "\n",
            "File ‘datasets/categories_dataset.csv’ already there; not retrieving.\n",
            "\n",
            "    category          word  score  ...  POS stza  POS nlkt  POS spcy\n",
            "0  confident     tolerance   2.63  ...      NOUN      NOUN      NOUN\n",
            "1  confident         carry   0.46  ...      VERB      NOUN      VERB\n",
            "2  confident   tranquility   1.00  ...      NOUN      NOUN      NOUN\n",
            "3  confident  socontagious   2.69  ...       ADJ       ADJ       ADJ\n",
            "4  confident          zeal   1.00  ...      NOUN      NOUN     PROPN\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "y: [ 1  1  1 ... 10 10 10]\n",
            "26 0\n",
            "\n",
            "x_train: \n",
            "[[ 5  6  6 ...  0  0  0]\n",
            " [13  9  4 ...  0  0  0]\n",
            " [19 16  1 ...  0  0  0]\n",
            " ...\n",
            " [20 18  1 ...  0  0  0]\n",
            " [ 4  1 25 ...  0  0  0]\n",
            " [14 21 18 ...  0  0  0]]\n",
            "\n",
            "y_train: \n",
            "[ 2  5  4 ... 10  9 10]\n",
            "(62870, 30) (18861, 30) (26945, 30)\n",
            "(62870,) (18861,) (26945, 30)\n",
            "[ 7  5  6 ... 10  1  4]\n",
            "\n",
            "building the model...\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 30, 64)            320064    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 30, 128)           66048     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 30, 128)           98816     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 11)                715       \n",
            "=================================================================\n",
            "Total params: 493,899\n",
            "Trainable params: 493,899\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS_cMXislcAr",
        "outputId": "ab577cd0-525f-48f0-b594-68b780d9bbb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# train the model\n",
        "history = train(model=model, x_train=x_train, y_train=to_categorical(y_train), batch_size=BATCH_SIZE,\n",
        "                x_validation=x_validation, y_validation=to_categorical(y_validation),\n",
        "                epochs=EPOCHS, verbose=1, monitor_es='val_loss', mode_es='min', restore=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training...\n",
            "\n",
            "Epoch 1/300\n",
            "1965/1965 [==============================] - ETA: 0s - loss: 2.1115 - accuracy: 0.2256 - auc: 0.7202 - precision: 0.0000e+00 - recall: 0.0000e+00WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1965/1965 [==============================] - 99s 50ms/step - loss: 2.1115 - accuracy: 0.2256 - auc: 0.7202 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.1021 - val_accuracy: 0.2266 - val_auc: 0.7257 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/300\n",
            "1964/1965 [============================>.] - ETA: 0s - loss: 2.0911 - accuracy: 0.2284 - auc: 0.7294 - precision: 0.0000e+00 - recall: 0.0000e+00WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1965/1965 [==============================] - 97s 49ms/step - loss: 2.0911 - accuracy: 0.2284 - auc: 0.7294 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.0959 - val_accuracy: 0.2305 - val_auc: 0.7285 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/300\n",
            "1964/1965 [============================>.] - ETA: 0s - loss: 2.0825 - accuracy: 0.2319 - auc: 0.7333 - precision: 0.0000e+00 - recall: 0.0000e+00WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1965/1965 [==============================] - 96s 49ms/step - loss: 2.0826 - accuracy: 0.2319 - auc: 0.7333 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.0822 - val_accuracy: 0.2323 - val_auc: 0.7342 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/300\n",
            "1964/1965 [============================>.] - ETA: 0s - loss: 2.0711 - accuracy: 0.2377 - auc: 0.7380 - precision: 0.0000e+00 - recall: 0.0000e+00WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1965/1965 [==============================] - 97s 49ms/step - loss: 2.0711 - accuracy: 0.2377 - auc: 0.7380 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.0662 - val_accuracy: 0.2393 - val_auc: 0.7414 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/300\n",
            "1964/1965 [============================>.] - ETA: 0s - loss: 2.0537 - accuracy: 0.2437 - auc: 0.7448 - precision: 0.2778 - recall: 7.9557e-05WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1965/1965 [==============================] - 96s 49ms/step - loss: 2.0537 - accuracy: 0.2437 - auc: 0.7449 - precision: 0.2778 - recall: 7.9529e-05 - val_loss: 2.0371 - val_accuracy: 0.2530 - val_auc: 0.7517 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/300\n",
            "1965/1965 [==============================] - ETA: 0s - loss: 2.0302 - accuracy: 0.2542 - auc: 0.7533 - precision: 0.3718 - recall: 4.6127e-04WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "1965/1965 [==============================] - 97s 49ms/step - loss: 2.0302 - accuracy: 0.2542 - auc: 0.7533 - precision: 0.3718 - recall: 4.6127e-04 - val_loss: 2.0105 - val_accuracy: 0.2622 - val_auc: 0.7609 - val_precision: 0.2941 - val_recall: 2.6510e-04\n",
            "Epoch 7/300\n",
            " 301/1965 [===>..........................] - ETA: 1:16 - loss: 2.0076 - accuracy: 0.2640 - auc: 0.7615 - precision: 0.3333 - recall: 5.1910e-04"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrrZbYi-jNXa"
      },
      "source": [
        "plot_history(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASc2rTOkjLKY"
      },
      "source": [
        "# evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test_val, y_test_val)\n",
        "evaluate_lst = list(map(lambda x: x * 100, [test_loss, test_accuracy]))\n",
        "print('\\nTest:\\nLoss: {}\\nAccuracy: {}').format(evaluate_lst[0], evaluate_lst[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol2Q8hOXjKfr"
      },
      "source": [
        "# predict\n",
        "s = 'love'\n",
        "pred = predict(label_dict, s, model)\n",
        "print('Word: {}'.format(s))\n",
        "print('Prediction: {}'.format(pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbh8T-mrx20u"
      },
      "source": [
        "# evaluation confusion matrix\n",
        "tf.math.confusion_matrix(labels=label_dict, predictions=x_validation) # what X prediction should I use here? "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TlxepYljOxE"
      },
      "source": [
        "# save the model\n",
        "!mkdir models\n",
        "model.save(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp7RGZe-jPDB"
      },
      "source": [
        "# convert to modeljs\n",
        "tfjs.converters.save_keras_model(model, 'tfjsmodel')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}