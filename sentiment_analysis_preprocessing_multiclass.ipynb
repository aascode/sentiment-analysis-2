{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment analysis preprocessing multiclass.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TukimOu3IeLK",
        "colab_type": "text"
      },
      "source": [
        "## some documentation to check\n",
        "- process slang:\n",
        "    * https://github.com/vi3k6i5/flashtext1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ti03vHymBWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# labels\n",
        "highlights = {\n",
        "    # related with speech recognition\n",
        "    'professional qualities': ['handles pressure'],\n",
        "    'soft skills': ['silence'],\n",
        "    'answer analysis': ['filler words', 'long pause', 'focus', 'patience'], \n",
        "\n",
        "    'polarities': {\n",
        "        'negative': [\n",
        "                     # confidence\n",
        "                     'not confident', \n",
        "                     'unsure',\n",
        "\n",
        "                     # professional qualities\n",
        "                     'does not handle pressure',\n",
        "                     'disordered',\n",
        "                     'talks to much',\n",
        "                     'uninsterested', #\n",
        "\n",
        "                     # soft skills\n",
        "                     'sad',\n",
        "                     'unfriendly'\n",
        "                     ],\n",
        "\n",
        "        'positive': [\n",
        "                     # confidence\n",
        "                     'confident', \n",
        "                     'certain',\n",
        "\n",
        "                     # professional qualities\n",
        "                     'handles pressure',\n",
        "                     'organized',\n",
        "                     'concise', \n",
        "                     'interested', # 'engaged'\n",
        "\n",
        "                     # soft skills\n",
        "                     'happy',\n",
        "                     'friendly'\n",
        "                     ]\n",
        "    }\n",
        "}\n",
        "\n",
        "main_lst = list(highlights.values())\n",
        "main_labels = [k for j in main_lst for k in j]\n",
        "\n",
        "neg_pos_lst = highlights['polarities'].values()\n",
        "neg_pos_labels = [k for j in neg_pos_lst for k in j]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIuwykirmIbc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "78fd702d-5516-4fc2-a97b-c744043f3735"
      },
      "source": [
        "neg_pos_lst"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_values([['not confident', 'unsure', 'does not handle pressure', 'disordered', 'talks to much', 'uninsterested', 'sad', 'unfriendly'], ['confident', 'certain', 'handles pressure', 'organized', 'concise', 'interested', 'happy', 'friendly']])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug78ZM2qmJOp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "aac3e639-c327-43a2-e202-056bb2d5e405"
      },
      "source": [
        "neg_pos_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['not confident',\n",
              " 'unsure',\n",
              " 'does not handle pressure',\n",
              " 'disordered',\n",
              " 'talks to much',\n",
              " 'uninsterested',\n",
              " 'sad',\n",
              " 'unfriendly',\n",
              " 'confident',\n",
              " 'certain',\n",
              " 'handles pressure',\n",
              " 'organized',\n",
              " 'concise',\n",
              " 'interested',\n",
              " 'happy',\n",
              " 'friendly']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH-eGFRwh5ax",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **1. Installations**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Wliixqgmo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk \n",
        "!pip install stanza\n",
        "!pip install spacy\n",
        "!pip install --upgrade git+https://github.com/flairNLP/flair.git\n",
        "!pip install textblob\n",
        "\n",
        "!pip install emoji --upgrade\n",
        "\n",
        "!spacy download en_core_web_sm # sm md lg\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKQbIeZkiJ6t",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **2. Imports and downloads**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOcRIcOiiUu9",
        "colab_type": "text"
      },
      "source": [
        "## Core"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAIxYRUhiOWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "import time\n",
        "\n",
        "from emoji import demojize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ombh-iahiXoc",
        "colab_type": "text"
      },
      "source": [
        "## NLTK\n",
        "* Words\n",
        "* Stopwords\n",
        "* WordNetLemmatizer\n",
        "* Vader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixlc1Gg2gV6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "3e707e33-7e83-413f-9bfb-6b9679bddfa9"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# words\n",
        "NLTK_WORDS = set(words.words())\n",
        "\n",
        "# Stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# Vader\n",
        "SIA = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpI3ZBfQjAoW",
        "colab_type": "text"
      },
      "source": [
        "## Stanza\n",
        "* ewt; tokenize, mwt, pos, lemma\n",
        "* default; tokenize, sentiment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBsjg0d5gde0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "outputId": "8b9ce483-bb27-4bd5-8a65-25ab8db0210b"
      },
      "source": [
        "import stanza\n",
        "\n",
        "stanza.download('en', package='ewt', processors='tokenize,mwt,pos,lemma', verbose=True)\n",
        "stanza.download('en', package='default', processors='tokenize,sentiment', verbose=True)\n",
        "\n",
        "stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma,sentiment',\n",
        "                      lang='en',\n",
        "                      use_gpu=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 13.3MB/s]                    \n",
            "2020-09-03 01:48:33 WARNING: Can not find mwt: ewt from official model list. Ignoring it.\n",
            "2020-09-03 01:48:33 INFO: Downloading these customized packages for language: en (English)...\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "| pretrain  | ewt     |\n",
            "=======================\n",
            "\n",
            "2020-09-03 01:48:33 INFO: File exists: /root/stanza_resources/en/tokenize/ewt.pt.\n",
            "2020-09-03 01:48:33 INFO: File exists: /root/stanza_resources/en/pos/ewt.pt.\n",
            "2020-09-03 01:48:33 INFO: File exists: /root/stanza_resources/en/lemma/ewt.pt.\n",
            "2020-09-03 01:48:34 INFO: File exists: /root/stanza_resources/en/pretrain/ewt.pt.\n",
            "2020-09-03 01:48:34 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 9.56MB/s]                    \n",
            "2020-09-03 01:48:34 INFO: Downloading these customized packages for language: en (English)...\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| sentiment | sstplus |\n",
            "| pretrain  | ewt     |\n",
            "=======================\n",
            "\n",
            "2020-09-03 01:48:34 INFO: File exists: /root/stanza_resources/en/tokenize/ewt.pt.\n",
            "2020-09-03 01:48:34 INFO: File exists: /root/stanza_resources/en/sentiment/sstplus.pt.\n",
            "2020-09-03 01:48:34 INFO: File exists: /root/stanza_resources/en/pretrain/ewt.pt.\n",
            "2020-09-03 01:48:34 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2020-09-03 01:48:34 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
            "2020-09-03 01:48:34 INFO: Loading these models for language: en (English):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "| lemma     | ewt     |\n",
            "| sentiment | sstplus |\n",
            "=======================\n",
            "\n",
            "2020-09-03 01:48:34 INFO: Use device: cpu\n",
            "2020-09-03 01:48:34 INFO: Loading: tokenize\n",
            "2020-09-03 01:48:34 INFO: Loading: pos\n",
            "2020-09-03 01:48:36 INFO: Loading: lemma\n",
            "2020-09-03 01:48:36 INFO: Loading: sentiment\n",
            "2020-09-03 01:48:37 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdzOmfrbjl_d",
        "colab_type": "text"
      },
      "source": [
        "## SpaCy\n",
        "* en_core_web sm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-w_t_p3ghAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "spNLP = spacy.load('en_core_web_sm')\n",
        "spNLP.max_length = 103950039 # or higher\n",
        "# spacy.prefer_gpu() #will not work with stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eKhFCbQkQn8",
        "colab_type": "text"
      },
      "source": [
        "## TextBlob\n",
        "* use a bag of words classifier, but the advantage is that it includes subjetivity analysis (factual/opinated)\n",
        "* it doesn't contain the heuristics that nltk has, it won't intensify or negate a sentence's sentiment\n",
        "\n",
        "* will return the subjectivity of the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMXi5Ab0kSyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA_FOj-1lxc1",
        "colab_type": "text"
      },
      "source": [
        "## **Flair**\n",
        "* classifier based on a character-leval LSTM. Takes a sequences of letters and words into account when predicting\n",
        "\n",
        "* one of its biggest advantages is that it can predict a sentiment for OOV words that it has never seen before too (such as typos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59AxHZvOl02c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e3309bb-0f03-4aa7-a061-7d8e746aeac6"
      },
      "source": [
        "import flair\n",
        "flair_sent = flair.models.TextClassifier.load('en-sentiment') # fast-sentiment "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-03 01:48:41,080 loading file /root/.flair/models/sentiment-en-mix-distillbert.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65jn8kxAixOf",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **3. Functions**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P_X4V-omPLz",
        "colab_type": "text"
      },
      "source": [
        "## **i. Lemmatizers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw0tfN636u4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nltk_lemma(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatizer.lemmatize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjVJN1YM610R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stanza\n",
        "def stanza_lemma(text):\n",
        "    doc = stNLP(text)\n",
        "    return ' '.join([word.lemma for sent in doc.sentences for word in sent.words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_g-MC8BmlPq",
        "colab_type": "text"
      },
      "source": [
        "## ii. Sentiment Analyzers\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze-Ry3MdnJd8",
        "colab_type": "text"
      },
      "source": [
        "### NLTK Vader\n",
        "* VADER, has different ratings depending on the form of the word and therefore the input should not be stemmed or lemmatized.\n",
        "\n",
        "* disadvantage of this approach is that Out of Vocab (OOV) words that the sentiment analysis tool has not seen before will not be classified as positive/negative (e.g. typos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaoEIpfAmn3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def siaVader_compound(text):\n",
        "    scores = SIA.polarity_scores(text)\n",
        "    \n",
        "    comp_score = scores['compound']\n",
        "    if comp_score >= 0.05:\n",
        "        str_comp = 'pos'\n",
        "    elif comp_score <= -0.05:\n",
        "        str_comp = 'neg'\n",
        "    else: # (compound score > -0.05) and (compound score < 0.05)\n",
        "        str_comp = 'neu'\n",
        "    return str_comp\n",
        "\n",
        "def siaVader_maxScore(text):\n",
        "    scores = SIA.polarity_scores(text)\n",
        "    \n",
        "    del scores['compound']\n",
        "    index = np.argmax(list(scores.values()))\n",
        "    vader_MaxScore = list(scores.values())[index]\n",
        "    vader_label = list(scores)[index]\n",
        "    \n",
        "    return vader_label\n",
        "\n",
        "###\n",
        "def siaVader_byWord(text):\n",
        "    c = 0\n",
        "    for n, y in enumerate(text):\n",
        "        x = SIA.polarity_scores(y)\n",
        "        if x['compound'] != 0.0:\n",
        "            c += 1\n",
        "            # print('{}. {} {}'.format(c, x, y))\n",
        "            return 'pos' if x > 0.05 else 'neg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp4xW7dnnNsT",
        "colab_type": "text"
      },
      "source": [
        "### **TextBlob**\n",
        "* use a bag of words classifier, but the advantage is that it includes subjetivity analysis (factual/opinated)\n",
        "* it doesn't contain the heuristics that nltk has, it won't intensify or negate a sentence's sentiment\n",
        "\n",
        "* will return the subjectivity of the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkfcolyImpQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_blob_subject(text):\n",
        "    return TextBlob(text).sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsvP-jgenSqD",
        "colab_type": "text"
      },
      "source": [
        "### **Flair LSTM**\n",
        "* classifier based on a character-leval LSTM. Takes a sequences of letters and words into account when predicting\n",
        "\n",
        "* one of its biggest advantages is that it can predict a sentiment for OOV words that it has never seen before too (such as typos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR0Ic6OkmtKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flair_lstm(text):\n",
        "    sentence = flair.data.Sentence(text)\n",
        "    flair_sent.predict(sentences=sentence)\n",
        "    total_sent = sentence.labels\n",
        "    for label in total_sent:\n",
        "        value = label.value\n",
        "        score = label.score\n",
        "        return '1' if value == 'POSITIVE' else '-1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlxoeIP9o6ZN",
        "colab_type": "text"
      },
      "source": [
        "### **Stanza**\n",
        "* stanza pipeline by using a CNN classifier.\n",
        "* training this model on 2 class data using higher dimension word vectors achieves the 87 score reported in the original CNN classifier paper. On a three class projection of the SST test data, the model trained on multiple datasets gets 70.0%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFcUZ67ZpC6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stanza_polarity(text):\n",
        "    try:\n",
        "        data = stNLP(text)\n",
        "        for sentence in data.sentences:\n",
        "            with open('debug.txt', 'a') as f:\n",
        "                f.write('\\n')\n",
        "                f.write(text)\n",
        "            return sentence.sentiment\n",
        "    except:\n",
        "        print('...\\n')\n",
        "        print(text)\n",
        "        print('...\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo-atzwUmZdy",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## **iii. Load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FbDdGQcwqPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_path=None):\n",
        "    print('load the dataset...\\n')\n",
        "    !mkdir -p data\n",
        "    !wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip -P data\n",
        "    !unzip -n -d data data/training.1600000.processed.noemoticon.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmCEUmBPmbia",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **4. Preprocess dataset**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I71x7SvXwmH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_dataset(PATH_FILE, index_col=None):\n",
        "    print('preprocess the dataset...\\n')\n",
        "\n",
        "    # load_data\n",
        "    load_data()\n",
        "    print('Database loaded\\n')\n",
        "\n",
        "    # cleaning data\n",
        "    unclean_df = pd.read_csv(PATH_FILE,\n",
        "                     names=['polarity', 'id', 'date', 'query', 'user', 'text'],\n",
        "                     encoding='latin-1') # if utf-8: UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 232719-232720: invalid continuation byte\n",
        "\n",
        "    # replace polarity\n",
        "    unclean_df.polarity = unclean_df.polarity.replace({0: 0, 4: 1}) \n",
        "    \n",
        "    # dropping unneeded columns\n",
        "    unclean_df = unclean_df.drop(columns=['id', 'date', 'query', 'user']) \n",
        "\n",
        "    # lower case\n",
        "    unclean_df['text'] = unclean_df['text'].str.lower()\n",
        "\n",
        "    # removing urls\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'https://www\\.|http:\\.|https://|www\\.', '', x))\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil|cl)[\\S]*\\s?', '', x))\n",
        "\n",
        "    # remove special character and numbers\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-zÁ-Úá-ú \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?%', '', x))\n",
        "    unclean_df['text'] = unclean_df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "    # remove repetitions (goood ==> good ==> god?; whaaat ==> what)\n",
        "    pattern = re.compile(r'(.)\\1{2,}', re.DOTALL)\n",
        "    unclean_df['text'] = unclean_df['text'].str.replace(pattern, r'\\1')\n",
        "\n",
        "    # removing empty values\n",
        "    nan_value = float('NaN')\n",
        "    unclean_df.replace('', nan_value, inplace=True)\n",
        "    unclean_df.replace(' ', nan_value, inplace=True) # added\n",
        "    unclean_df.dropna(inplace=True) # add subset\n",
        "\n",
        "    # removing stopwords\n",
        "    #df['text'] = df['text'].apply(lambda x: ' '.join([i for i in x.split() if i not in (STOPWORDS)]))\n",
        "\n",
        "    # filtering and removing non-english words or misspelling\n",
        "    #df['text'] = df['text'].apply(lambda x: ' '.join([i for i in x.split() if i.lower() in NLTK_WORDS or not i.isalpha()]))\n",
        "\n",
        "    # rewritting the created file without NaN values\n",
        "    unclean_df.to_csv('data/sentiment140-subset.csv',\n",
        "              quotechar='\"', # check later!\n",
        "              encoding='utf-8',\n",
        "              index=False)\n",
        "\n",
        "    # clean csv\n",
        "    df = pd.read_csv('data/sentiment140-subset.csv', encoding='utf-8', warn_bad_lines=True).dropna()\n",
        "\n",
        "    # checking if there's any NaN values\n",
        "    isnull = [i for i in (df['text'].isnull()) if i == True]\n",
        "    if isnull != []:\n",
        "        sys.exit(0) # add response object here\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CinwlW5wxvxn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "bae75244-0b24-4470-ab8d-e30edd7305f0"
      },
      "source": [
        "df = preprocess_dataset(PATH_FILE='data/training.1600000.processed.noemoticon.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocess the dataset...\n",
            "\n",
            "load the dataset...\n",
            "\n",
            "File ‘data/training.1600000.processed.noemoticon.csv.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  data/training.1600000.processed.noemoticon.csv.zip\n",
            "Database loaded\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iREYVprZwLRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from emoji import demojize, emojize\n",
        "# removing emojis\n",
        "#df['text'] = df['text'].apply(lambda x: demojize(string=x))\n",
        "#df_emojis = df['text'].apply(lambda x: re.findall(r':[a-z_]+:', string=demojize(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTolK81BQnvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for n, i in enumerate(df.emojis):\n",
        "#    if i != []:\n",
        "#        print(n, emojize(str(i)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ap_AgDrDhUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_csv = df.to_csv('checking_csv.csv', quotechar='\"', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5U88c3AyhbO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "2a83f3e0-90c3-4fb0-a71a-2625ce4c1cd7"
      },
      "source": [
        "'''\n",
        "emo_dict = {}\n",
        "for i, j in df_emojis.iteritems():\n",
        "    for k in j:\n",
        "        if k in emo_dict:\n",
        "            emo_dict[k] += 1\n",
        "        else:\n",
        "            emo_dict[k] = 1\n",
        "\n",
        "df_hashtags = df['text'].apply(lambda x: re.findall(r'#/S+', string=x))\n",
        "hashtags = {}\n",
        "for i, j in df_hashtags.iteritems(): \n",
        "    for k in j:\n",
        "        if k in hashtags:\n",
        "            hashtags[k] += 1\n",
        "        else:\n",
        "            hashtags[k] = 1\n",
        "            \n",
        "for i, c in sorted(emo_dict.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(emojize(i) + i + str(c))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nemo_dict = {}\\nfor i, j in df_emojis.iteritems():\\n    for k in j:\\n        if k in emo_dict:\\n            emo_dict[k] += 1\\n        else:\\n            emo_dict[k] = 1\\n\\ndf_hashtags = df['text'].apply(lambda x: re.findall(r'#/S+', string=x))\\nhashtags = {}\\nfor i, j in df_hashtags.iteritems(): \\n    for k in j:\\n        if k in hashtags:\\n            hashtags[k] += 1\\n        else:\\n            hashtags[k] = 1\\n            \\nfor i, c in sorted(emo_dict.items(), key=lambda x: x[1], reverse=True):\\n    print(emojize(i) + i + str(c))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1Me-JFlxrhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing\n",
        "df_test = pd.read_csv('emo_test.csv', sep='\\t')\n",
        "df_test['SIA'] = df_test['word'].apply(lambda x: siaVader_compound(x))\n",
        "df_test['comp_val'] = df_test['word'].apply(lambda x: (SIA.polarity_scores(x))['compound'])\n",
        "df_test['VADER_pos_neg'] = df_test['word'].apply(\n",
        "    lambda x: SIA.polarity_scores(x)['pos'] if SIA.polarity_scores(x)['pos'] > SIA.polarity_scores(x)['neg'] else SIA.polarity_scores(x)['neg']\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnWQErU43V-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.models import TextClassifier\n",
        "from flair.embeddings import TransformerDocumentEmbeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "friq1FCSy8GT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test['flair'] = df_test['word'].apply(lambda x: flair_lstm(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3Gb3jQBpxa0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "56730519-c111-472a-e55f-19c83df2fa3c"
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>emotion</th>\n",
              "      <th>emotion-intensity-score</th>\n",
              "      <th>SIA</th>\n",
              "      <th>comp_val</th>\n",
              "      <th>VADER_pos_neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>outraged</td>\n",
              "      <td>anger</td>\n",
              "      <td>0.964</td>\n",
              "      <td>neg</td>\n",
              "      <td>-0.5423</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brutality</td>\n",
              "      <td>anger</td>\n",
              "      <td>0.959</td>\n",
              "      <td>neg</td>\n",
              "      <td>-0.6124</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hatred</td>\n",
              "      <td>anger</td>\n",
              "      <td>0.953</td>\n",
              "      <td>neg</td>\n",
              "      <td>-0.6369</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hateful</td>\n",
              "      <td>anger</td>\n",
              "      <td>0.940</td>\n",
              "      <td>neg</td>\n",
              "      <td>-0.4939</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>terrorize</td>\n",
              "      <td>anger</td>\n",
              "      <td>0.939</td>\n",
              "      <td>neg</td>\n",
              "      <td>-0.6486</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        word emotion  emotion-intensity-score  SIA  comp_val  VADER_pos_neg\n",
              "0   outraged   anger                    0.964  neg   -0.5423            1.0\n",
              "1  brutality   anger                    0.959  neg   -0.6124            1.0\n",
              "2     hatred   anger                    0.953  neg   -0.6369            1.0\n",
              "3    hateful   anger                    0.940  neg   -0.4939            1.0\n",
              "4  terrorize   anger                    0.939  neg   -0.6486            1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmDmGBmblTmc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "da6e1755-8479-4440-975e-d15a52898dfd"
      },
      "source": [
        "!transformers-cli env"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-03 01:49:45.326917: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/transformers/commands/env.py:36: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "2020-09-03 01:49:47.637917: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-09-03 01:49:47.638381: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6860680 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-09-03 01:49:47.638415: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-09-03 01:49:47.640971: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-09-03 01:49:47.644481: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2020-09-03 01:49:47.644528: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (e570412d2d18): /proc/driver/nvidia/version does not exist\n",
            "\n",
            "Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n",
            "\n",
            "- `transformers` version: 3.1.0\n",
            "- Platform: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\n",
            "- Python version: 3.6.9\n",
            "- PyTorch version (GPU?): 1.6.0+cu101 (False)\n",
            "- Tensorflow version (GPU?): 2.3.0 (False)\n",
            "- Using GPU in script?: <fill in>\n",
            "- Using distributed or parallel set-up in script?: <fill in>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm3Li3dxGLoN",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **5. Classification with polarities, applying compound, flair, TextBlob (subjectivity) and Stanza**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atxY1vkAof8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNQ_bFhmw96z",
        "colab_type": "text"
      },
      "source": [
        "### Vader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8lMisAgMR0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Comp_label'] = df['text'].apply(lambda x: siaVader_compound(text=x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXT4vF2ry7w_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Comp_score'] = df['text'].apply(lambda x: SIA.polarity_scores(text=x)['compound'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj9fS-FpxBS_",
        "colab_type": "text"
      },
      "source": [
        "### TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHKWUkqAoHdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Subjectivity'] = df['text'].apply(lambda x: text_blob_subject(text=x)[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeERUfhZkp1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# removing empty values\n",
        "nan_value = float('NaN')\n",
        "df.replace('', nan_value, inplace=True)\n",
        "df.replace(' ', nan_value, inplace=True) # added\n",
        "df.replace('  ', nan_value, inplace=True)\n",
        "df.dropna(inplace=True) # add subset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbzh83bExlL8",
        "colab_type": "text"
      },
      "source": [
        "### Stanza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpTmS-tgxkaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Stanza'] = df['text'].apply(lambda x: stanza_polarity(text=x))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNEQSsWkxKgU",
        "colab_type": "text"
      },
      "source": [
        "### Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ychJGKqt1rTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['flair'] = df['text'].apply(lambda x: flair_lstm(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiQvohY0GuZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elapsed = time.time() - start_time\n",
        "elapsed_minutes = elapsed / 60\n",
        "elapsed_minutes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5VE3i30xOfN",
        "colab_type": "text"
      },
      "source": [
        "### Sorting csv new rows "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An0SHMsWFHFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.sort_values(by=['Stanza'], ascending=True, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8D7huDHurvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tvot1uQyLpv",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **6. Graphs**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxF9K6HByczf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotting(x, size, p1y, p2y, p3y, p4y):\n",
        "    plt.figure(figsize=size)\n",
        "    plt.plot(x, p1y, linestyle='solid', color='red', label='Flair')\n",
        "    plt.plot(x, p2y, linestyle='dashed', color='purple', label='Subjectivity')\n",
        "    plt.plot(x, p3y, linestyle='solid', color='green', label='Vader')\n",
        "    plt.plot(x, p4y, linestyle='solid', color='black', label='Stanza')\n",
        "    plt.title('NLP SCORES ON DATABASE')\n",
        "    plt.xlabel('TWEETS')\n",
        "    plt.ylabel('SCORES')\n",
        "    plt.legend()\n",
        "    plt.xticks(size=10, rotation='vertical')\n",
        "    plt.yticks(size=10)\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NZ9kN6iySg-",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **7. Clean data to csv**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag0SHAvrFEjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('dataset_clean.csv', encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}